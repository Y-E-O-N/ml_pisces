{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[차원 축소]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원 축소는 크게 아래 두 가지로 나눌 수 있다.\n",
    "- 선형 투영(linear projection)\n",
    "- 비선형 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 투영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.1 주성분 분석 (PCA; Principal Component Analysis)\n",
    "\n",
    "* 최대한 다양성을 유지하면서 데이터의 저차원 표현을 찾는 방법\n",
    "* 분산 중 일부 손실\n",
    "* 데이터 내재 구조 식별 용이, 클러스터링 효율적 수행 가능\n",
    "* 예) 점진적 PCA, 커널 PCA, 희소 PCA\n",
    "\n",
    "\t- 점진적 PCA: 데이터셋의 크기가 매우 커서 메모리에 저장할 수 없는 경우, 메모리에 저장되는 크기로 배치를 작게 설정해 점진적으로 PCA를 수행할 수 있습니다. 배치 크기는 수동으로 설정하거나 자동으로 결정할 수 있습니다. 배치 기반 PCA 유형입니다. 주성분 결과는 일반 PCA와 유사합니다\n",
    "\n",
    "\t- 희소 PCA: 일반 PCA 알고리즘은 모든 입력 변수에서 선형 결합ㅇ르 탐색해 원본 피처 공간을 최대한 조밀하게 줄입니다. 그러다 일부 머신러닝 문제의 경우, 어느 정도의 희소성이 선호될 수 있습니다. 알파라는 하이퍼파라미터로 제어함으로써 희소성을 어느 정도 유지하는 PCA 버전을 희소 PCA라고 합니다. 희소 PCA 알고리즘은 일부 입력 변수에서만 선형 결합을 탐색해 원본 피처 공간을 어느 정도 줄이지만 일반 PCA만큼 조밀하게 만들지는 않습니다.\n",
    "\n",
    "\t- 커널 PCA: 원본 데이터 포인트 쌍들에 대해 유사성 함수를 실행시켜 비선형적으로 차원을 축소합니다.  이 유사성 함수(커널 기법)를 학습함으로써 데이터 포인트 대부분이 있는 암시적 피처 공간을 매핑하고 이 공간을 원본 피처셋보다 훨씬 더 적은 수의 차원으로 만듭니다. 이 방법은 원본 피처셋을 선형으로 분리할 수 없는 경우에 효과적입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 특잇값 분해 (SVD; Singular Value Decomposition)\n",
    "\n",
    "* 원래 행렬의 차원을 작은 차원으로 줄이는 방법. \n",
    "* 일부 벡터의 선형 결합을 이용해 원래 행렬을 다시 만들 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 랜덤 투영 (Random Projection)\n",
    "\n",
    "* 예) 랜덤 가우시안 행렬 또는 랜덤 희소 행렬 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 비선형 차원 축소 (매니폴드 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 isomap\n",
    "\n",
    "😆 종이를 구겼을 때, 구겨진 종이 위의 점들을 다시 평평하게 펴는 것처럼, 복잡한 데이터를 더 이해하기 쉽게 펼쳐주는 방법이에요!\n",
    "* 고차원의 공간을 저차원 공간에 투영\n",
    "* 유클리드 거리가 아닌 지오데식(geodesic) 또는 곡선 거리로 모든 데이터 포인트 간의 쌍별 거리를 계산해 원본 피처셋의 새로운 저차원 임베딩 학습\n",
    "* 매니폴드 공간에서 각 포인트들과 이웃하는 포인트들 간의 상대적인 위치를 기반으로 원본 데이터의 고유한 기하학 구조 학습습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ISOMAP의 3단계 작동 원리:\n",
    "\n",
    "1) 이웃 그래프 구성\n",
    "\n",
    "- 각 데이터 포인트에 대해 k-최근접 이웃을 찾음\n",
    "- 또는 ε-반경 내의 모든 이웃을 찾음\n",
    "- 이웃 간 연결을 통해 그래프 구성\n",
    "- 이웃 사이의 거리는 유클리드 거리로 계산\n",
    "\n",
    "2) 최단 경로 계산\n",
    "\n",
    "- 구성된 그래프에서 모든 점들 사이의 최단 경로 계산\n",
    "- 주로 다익스트라(Dijkstra) 또는 플로이드-워셜(Floyd-Warshall) 알고리즘 사용\n",
    "- 이 최단 경로가 실제 측지 거리의 근사값이 됨\n",
    "\n",
    "\n",
    "3) 차원 축소 적용\n",
    "\n",
    "- 계산된 거리 행렬을 이용해 고유값 분해(eigenvalue decomposition) 수행\n",
    "- MDS(다차원척도법)를 적용하여 저차원 공간으로 매핑\n",
    "- 원본 데이터의 기하학적 구조 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[장점]**\n",
    "\n",
    "* 비선형 구조를 잘 포착\n",
    "* 전역적 기하 구조 보존\n",
    "* 직관적인 이해 가능\n",
    "\n",
    "**[단점]**\n",
    "\n",
    "* 계산 비용이 큼 (N개의 데이터 포인트에 대해 O(N²) ~ O(N³)의 시간 복잡도를 가짐)\n",
    "* 이웃 수(k) 선택에 민감 (적절한 k 값은 데이터마다 다르며, 이를 찾기 위해 여러 번의 시도가 필요)\n",
    "* 구멍이 있는 매니폴드에 취약 (매니폴드에 구멍이나 단절된 부분이 있으면 거리 계산이 부정확해짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[활용 예시]\n",
    "\n",
    "* 얼굴 인식, 음성 데이터 분석, 자연어 처리, 생물정보학 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 t-분포 확률적 임베딩 (t-SNE; t-distributed Stochastic Neighbor Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🟡 **쿨백-라이블러 발산(Kullback-Leibler Divergence, KL Divergence)**\n",
    "\n",
    "😆 두 개의 서로 다른 주사위가 있다고 생각해보세요. 이 주사위들이 얼마나 다르게 숫자가 나오는지 비교하는 방법이에요. 마치 두 주사위의 \"다름의 정도\"를 숫자로 나타내는 거죠!\n",
    "* KL 발산은 두 확률 분호 간의 차이를 측정하는 방법\n",
    "* 한 확률 분포가 다른 확률 분포로부터 얼마나 다른지를 나타내는 비대칭적 측도\n",
    "* '상대 엔트로피'라고도 부름\n",
    "\n",
    "[주요 특징]\n",
    "\n",
    "1) 비대칭성 (방향성에 따라 달라짐짐)\n",
    "\n",
    "-  D(P||Q) ≠ D(Q||P)\n",
    "- 기준 분포에 따라 값이 달라짐\n",
    "\n",
    "2) 항상 0 이상\n",
    "\n",
    "- 두 분포가 완전히 같으면 0\n",
    "- 다를수록 값이 커짐\n",
    "\n",
    "3) 주요 활용 분야\n",
    "\n",
    "- 머신러닝의 손실 함수\n",
    "- 변분 추론\n",
    "- 정보 이득 계산\n",
    "- 분포 매칭 문제\n",
    "\n",
    "[활용 예시]\n",
    "- 딥러닝 : VAE(Variational AutoEncoder), GAN(Generative Adversarial Network),  모델 압축/증류\n",
    "- 자연어 처리 : 문서 유사도 측정, 토픽 모델링\n",
    "- 강화 학습 : 정책 최적화, 행동 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) D(P||Q): P를 실제/목표 분포, Q를 근사/예측 분포로 볼 때\n",
    "- \"실제에서 예측을 보는 관점\"\n",
    "- 실제 분포의 중요한 부분을 놓치면 페널티가 큼\n",
    "- 예: 실제 유저의 취향(P)과 추천 시스템의 예측(Q) 비교\n",
    "\n",
    "\n",
    "2) D(Q||P): Q를 실제/목표 분포, P를 근사/예측 분포로 볼 때\n",
    "\n",
    "- \"예측에서 실제를 보는 관점\"\n",
    "- 예측이 실제에 없는 부분을 포함하면 페널티가 큼\n",
    "- 예: 추천 시스템의 예측(Q)과 실제 유저의 취향(P) 비교\n",
    "\n",
    "**실제 적용 시 선택 기준:**\n",
    "\n",
    "* D(P||Q): 실제 데이터의 중요한 패턴을 놓치면 안 될 때\n",
    "* D(Q||P): 예측이 지나치게 민감하게 반응하면 안 될 때\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 고차원 데이터를 2~3차원 공간에 투영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 JSE (Jensen-Shannpn Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😆두 점 사이의 관계를 볼 때, 양쪽 방향에서 모두 보고 그 중간값을 사용하는 방식이에요. 마치 두 사람이 서로를 바라보는 시각의 평균을 구하는 것처럼요\n",
    "\n",
    "* 데이터 포인트들 간의 JS발산을 보존하면서 차원 축소\n",
    "\n",
    "[장점]\n",
    "\n",
    "* 대칭적인 특성으로 더 안정적\n",
    "* 지역적 구조와 전역적 구조를 잘 보존\n",
    "* 이상치에 덜 민감\n",
    "* 군집 구조를 잘 보존\n",
    "\n",
    "[단점]\n",
    "\n",
    "* 계산 비용이 높음\n",
    "* 하이퍼파라미터 튜닝 필요\n",
    "* t-SNE 대비 덜 대중화화\n",
    "\n",
    "[수산물 데이터에 적용시]\n",
    "* 가격 패턴의 군집 구조 파악 유용\n",
    "* 안정적인 차원 축소 가능\n",
    "* 이상 가격 탐지에도 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🟡 **Jensen-Shannon Divergence(JSD)** : KL 발산의 평균을 사용\n",
    "\n",
    "\n",
    "JSD(P||Q) = 1/2 * D(P||M) + 1/2 * D(Q||M) \n",
    "\n",
    "(여기서 M = 1/2(P + Q))\n",
    "\n",
    "[장점]\n",
    "* 대칭적(방향성 문제 해결)\n",
    "* 항상 유한한 값을 가짐\n",
    "\t- 0 과 1 사이의 값 (로그의 밑이 2일 때)\n",
    "\t- KL발산은 무한대가 될 수 있음\n",
    "* 더 안정적인 학습\n",
    "\t- 극단적인 패널티 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😆 큰 그림을 작은 그림으로 줄였다가, 다시 원래 크기로 복원하는 것과 같아요. 이 과정에서 그림의 가장 중요한 부분을 배우게 되죠!\n",
    "* 입력 데이터를 압축(인코딩)했다가 다시 복원(디코딩)하는 과정을 통해 데이터의 중요한 특성을 학습하는 비지도 학습 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Autoencoder 구조 ]\n",
    "\n",
    "1. Encoder\n",
    "* 입력 데이터를 저차원으로 압축\n",
    "* 중요 특성 추출\n",
    "* 점진적으로 차원 감소\n",
    "\n",
    "2. Bottleneck\n",
    "* 가장 압축된 표현\n",
    "* 잠재 공간 (latent space)\n",
    "* 핵심 특성이 담김\n",
    "\n",
    "3. Decoder \n",
    "* 압축된 데이터를 원래 차원으로 복원\n",
    "* 인코더의 반대 구조\n",
    "* 점진적으로 차원 증가\n",
    "\n",
    "[ 장점 ]\n",
    "1) 비선형 관계 포착\n",
    "2) 노이즈 제거 효과\n",
    "3) 계절성 패턴 학습\n",
    "4) 새로운 데이터에 적용 용이\n",
    "5) end-to-end 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "😆 복잡하게 얽혀있는 데이터를 풀어서 간단한 그림으로 그리는 방법이에요. 마치 실타래를 풀어서 반듯하게 정리하는 것처럼요!\n",
    "* 위상수학(Topology)의 원리를 사용하여 고차원 데이터의 매니폴드 구조를 저차원에 투영하는 알고리즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[작동원리]\n",
    "* 지역 구조 파악\n",
    "\n",
    "\t- 각 데이터 포인트 주변의 이웃 찾기\n",
    "\t- 확률적 그래프 구성\n",
    "\t- 모든 점들의 연결 관계 정의\n",
    "\n",
    "* 전역 구조 보존\n",
    "\n",
    "\t- 리만 기하학 개념 활용\n",
    "\t- 매니폴드의 위상 구조 보존\n",
    "\t- 데이터의 전체적인 패턴 유지\n",
    "\n",
    "[장점]\n",
    "\n",
    "* 빠른 계산 속도\n",
    "* 메모리 효율성\n",
    "* 이론적 기반이 탄탄\n",
    "* 새로운 데이터 적용 가능\n",
    "* 군집 구조 잘 보존\n",
    "\n",
    "[단점]\n",
    "\n",
    "* 하이퍼파라미터 조정 필요\n",
    "* 결과 해석이 직관적이지 않을 수 있음\n",
    "* 반복 실행시 결과가 달라질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 지역 선형 임베딩 (LLE; Locally Linear Embedding)\n",
    "\n",
    "😆 각 점이 주변 점들과 어떤 관계를 가지고 있는지 기억했다가, 그 관계를 최대한 유지하면서 더 단순한 형태로 바꾸는 방법이에요!\n",
    "* 원본 피처 공간에서 축소된 공간으로 데이터 투영 시 지역 내 이웃과의 거리 유지\n",
    "* 데이터를 더 작은 성분(포인트들의 이웃)으로 분할하고 각 성분을 선형 임베딩으로 모델링해 원본 고차원 데이터에서 비선형 구조 발견\n",
    "\n",
    "[장점]\n",
    "\n",
    "* 지역적 구조를 잘 보존\n",
    "* 비선형 매니폴드 학습에 효과적\n",
    "* 최적화 과정이 비교적 단순\n",
    "* 하이퍼파라미터가 적음(주로 이웃 수 k만 지정)\n",
    "\n",
    "[단점]\n",
    "\n",
    "* 새로운 데이터 포인트에 대한 직접적인 매핑이 어려움\n",
    "* 데이터 밀도에 민감\n",
    "* 이상치에 취약\n",
    "* 대규모 데이터셋에서 계산 비용이 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 사전 학습(dictionary learning)\n",
    "\n",
    "* 데이터에 내재된 희소 표현 학습\n",
    "* 단순 이진 벡터(0,1)로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [결론]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 최종 추천:\n",
    "\n",
    "1순위: Autoencoder\n",
    "\n",
    "가격 예측이라는 목적에 가장 적합  \n",
    "새로운 데이터 처리 가능  \n",
    "비선형 관계 포착 우수  \n",
    "시계열 특성 보존 가능 (구조 변경으로)\n",
    "\n",
    "2순위: UMAP\n",
    "  \n",
    "빠른 처리 속도  \n",
    "구조 보존 우수  \n",
    "실시간 적용 가능  \n",
    "시계열 특성 보존 제한적 (기본 구조에서는 어려움)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Autoencoder의 시계열 처리]\n",
    "\n",
    "* 시간적 특성을 반영하는 구조 사용 가능\n",
    "\n",
    "\t- LSTM Autoencoder\n",
    "\t- CNN Autoencoder\n",
    "\t- Temporal Convolutional Network(TCN) 기반 Autoencoder\n",
    "\t- 이런 구조들은 시계열의 순서와 연속성을 명시적으로 학습할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "[UMAP의 시계열 처리]\n",
    "\n",
    "+ 기본 UMAP\n",
    "\n",
    "\t- 시간 정보를 직접적으로 처리하지 않음\n",
    "\t- 순서나 연속성 보존이 제한적\n",
    "\n",
    "\n",
    "+ 시계열 처리를 위한 변형\n",
    "\n",
    "\t- 시간 정보를 추가 피처로 포함\n",
    "\t- 시간적 근접성을 거리 메트릭에 반영\n",
    "\t- 하지만 이는 '꼼수'에 가까움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE와 JSE의 한계\n",
    "\n",
    "* 학습 후 새로운 데이터가 들어왔을 때 이를 변환하기 어려움\n",
    "* 가격 예측은 지속적으로 새로운 데이터를 처리해야 하는데, 이게 불가능\n",
    "* t-SNE와 JSE는 시간적 순서나 연속성을 보존하기 어려움\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE\n",
    "장점:\n",
    "\n",
    "\n",
    "지역적 구조 보존이 매우 뛰어남\n",
    "군집 패턴을 시각적으로 잘 표현\n",
    "단점:\n",
    "새로운 데이터에 적용 어려움\n",
    "전역적 구조 보존이 약함\n",
    "계산 비용이 높음\n",
    "수산물 적용시:\n",
    "비슷한 가격 패턴을 가진 그룹 발견에는 좋음\n",
    "실시간 예측에는 부적합\n",
    "훈련/테스트 분리가 어려움\n",
    "\n",
    "\n",
    "JSE\n",
    "장점:\n",
    "\n",
    "\n",
    "균형잡힌 구조 보존(지역/전역)\n",
    "안정적인 결과\n",
    "이상치에 덜 민감\n",
    "단점:\n",
    "계산 비용이 매우 높음\n",
    "구현이 복잡\n",
    "실제 적용 사례가 적음\n",
    "수산물 적용시:\n",
    "안정적인 패턴 파악 가능\n",
    "이상 가격 탐지에 유용\n",
    "실시간 처리는 어려움\n",
    "\n",
    "\n",
    "Autoencoder\n",
    "장점:\n",
    "\n",
    "\n",
    "비선형 관계 포착 뛰어남\n",
    "새로운 데이터에 적용 용이\n",
    "end-to-end 학습 가능\n",
    "노이즈 제거 효과\n",
    "단점:\n",
    "많은 양의 훈련 데이터 필요\n",
    "하이퍼파라미터 튜닝이 복잡\n",
    "과적합 위험\n",
    "수산물 적용시:\n",
    "복잡한 가격 패턴 학습 가능\n",
    "실시간 예측에 활용 가능\n",
    "계절성 파악 잘됨\n",
    "\n",
    "\n",
    "UMAP\n",
    "장점:\n",
    "\n",
    "\n",
    "빠른 계산 속도\n",
    "지역/전역 구조 모두 보존\n",
    "새로운 데이터 적용 가능\n",
    "단점:\n",
    "결과 해석이 어려울 수 있음\n",
    "하이퍼파라미터 민감\n",
    "반복 실행시 결과 변동\n",
    "수산물 적용시:\n",
    "빠른 처리 가능\n",
    "복잡한 가격 관계 파악 용이\n",
    "실시간 적용 가능"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
