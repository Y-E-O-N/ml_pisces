{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 필요 파일 목록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'nst_{fish}_trend_2025-01-30.csv'\n",
    "* 'forecast_agg.csv'\n",
    "* 'ikh_item_price_2025-01-30.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 경제지표 데이터 전처리\n",
    "\n",
    "filled_economic_indicators.csv 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. yfinance에서 데이터 불러오기\n",
    "\n",
    "* 입력 파일 :  없음\n",
    "* 출력 파일 : '_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the data:\n",
      "                USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "Date                                                                      \n",
      "2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "2015-01-06  1108.500000  1882.449951  47.930000  21.120001  1219.300049   \n",
      "2015-01-07  1097.300049  1883.829956  48.650002  19.309999  1210.599976   \n",
      "\n",
      "               Silver       MOVE  \n",
      "Date                              \n",
      "2015-01-01        NaN        NaN  \n",
      "2015-01-02  15.734000  70.000000  \n",
      "2015-01-05  16.179001  71.500000  \n",
      "2015-01-06  16.603001  84.900002  \n",
      "2015-01-07  16.510000  85.199997  \n",
      "\n",
      "Basic statistics:\n",
      "           USD/KRW        KOSPI          WTI          VIX         Gold  \\\n",
      "count  2626.000000  2625.000000  2625.000000  2625.000000  2625.000000   \n",
      "mean   1198.244361  2373.886004    62.117570    18.236381  1611.870780   \n",
      "std      95.503176   350.355287    18.080615     7.224219   403.104126   \n",
      "min    1053.729980  1457.640015   -37.630001     9.140000  1050.800049   \n",
      "25%    1125.462463  2070.540039    48.660000    13.340000  1263.900024   \n",
      "50%    1172.719971  2360.810059    60.139999    16.320000  1550.400024   \n",
      "75%    1272.287537  2571.090088    74.110001    21.280001  1875.400024   \n",
      "max    1473.520020  3305.209961   123.699997    82.690002  2788.500000   \n",
      "\n",
      "            Silver         MOVE  \n",
      "count  2625.000000  2625.000000  \n",
      "mean     20.172996    79.393345  \n",
      "std       4.835867    28.073940  \n",
      "min      11.735000    36.619999  \n",
      "25%      16.299000    56.220001  \n",
      "50%      18.097000    71.860001  \n",
      "75%      23.896000    98.730003  \n",
      "max      34.831001   182.639999  \n",
      "\n",
      "Data saved to _economic_indicators_.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\280239614.py:33: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_final = df_final.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "# 시작일과 종료일 설정\n",
    "start_date = '2015-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# 데이터를 저장할 빈 데이터프레임 생성\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# 각 지표의 티커 심볼 정의\n",
    "tickers = {\n",
    "    'USD/KRW': 'KRW=X',  # 원달러 환율\n",
    "    'KOSPI': '^KS11',  # KOSPI\n",
    "    'WTI': 'CL=F',  # WTI 원유 선물\n",
    "    'VIX': '^VIX',  # VIX 지수\n",
    "    'Gold': 'GC=F',  # 금 선물\n",
    "    'Silver': 'SI=F',  # 은 선물\n",
    "    'MOVE' : '^MOVE' # MOVE Index\n",
    "}\n",
    "\n",
    "# 각 티커에 대해 데이터 다운로드\n",
    "for name, ticker in tickers.items():\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
    "        # 종가만 사용\n",
    "        df_final[name] = df['Close']\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "\n",
    "# 결측치 처리\n",
    "df_final = df_final.fillna(method='ffill')\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# 기본 통계 확인\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_filename = '_economic_indicators_.csv'\n",
    "df_final.to_csv(output_filename)\n",
    "print(f\"\\nData saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. 경제지표 데이터 결측치 채우기 \n",
    "\n",
    "* 입력 파일 : '_economic_indicators_.csv'\n",
    "* 출력 파일 : 'filled_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터 샘플:\n",
      "        Date      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "0 2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "1 2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2 2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "3 2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "4 2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "      Silver  MOVE  \n",
      "0        NaN   NaN  \n",
      "1  15.734000  70.0  \n",
      "2  15.734000  70.0  \n",
      "3  15.734000  70.0  \n",
      "4  16.179001  71.5  \n",
      "\n",
      "결측치 확인:\n",
      "Date       0\n",
      "USD/KRW    0\n",
      "KOSPI      1\n",
      "WTI        1\n",
      "VIX        1\n",
      "Gold       1\n",
      "Silver     1\n",
      "MOVE       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('_economic_indicators_.csv', encoding='utf-8')\n",
    "\n",
    "# Date 컬럼을 datetime으로 변환\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 시작일과 마지막일 추출\n",
    "start_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "# 모든 날짜가 포함된 데이터프레임 생성\n",
    "all_dates = pd.DataFrame(\n",
    "    {'Date': pd.date_range(start_date, end_date, freq='D')}\n",
    ")\n",
    "\n",
    "# 기존 데이터와 병합\n",
    "filled_df = pd.merge(all_dates, df, on='Date', how='left')\n",
    "\n",
    "# 결측치를 이전 값으로 채우기\n",
    "filled_df = filled_df.ffill()\n",
    "\n",
    "# 결과 저장\n",
    "filled_df.to_csv('filled_economic_indicators.csv', index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"처리된 데이터 샘플:\")\n",
    "print(filled_df.head())\n",
    "print(\"\\n결측치 확인:\")\n",
    "print(filled_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. 어종별 경제지표 칼럼 생성\n",
    "\n",
    "* 입력 파일 : 'filled_economic_indicators_.csv'\n",
    "* 출력 파일 : 'expanded_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 변경된 데이터 (상위 5개 행):\n",
      "         Date      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
      "0  2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
      "1  2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "2  2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "3  2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
      "4  2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "      Silver  MOVE     광어_KOSPI   광어_USD/KRW  ...      우럭_Gold  우럭_Silver  \\\n",
      "0        NaN   NaN          NaN  1092.699951  ...          NaN        NaN   \n",
      "1  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "2  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "3  15.734000  70.0  1926.439941  1093.599976  ...  1186.000000  15.734000   \n",
      "4  16.179001  71.5  1915.750000  1111.000000  ...  1203.900024  16.179001   \n",
      "\n",
      "   우럭_MOVE     참돔_KOSPI   참돔_USD/KRW     참돔_WTI     참돔_VIX      참돔_Gold  \\\n",
      "0      NaN          NaN  1092.699951        NaN        NaN          NaN   \n",
      "1     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "2     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "3     70.0  1926.439941  1093.599976  52.689999  17.790001  1186.000000   \n",
      "4     71.5  1915.750000  1111.000000  50.040001  19.920000  1203.900024   \n",
      "\n",
      "   참돔_Silver  참돔_MOVE  \n",
      "0        NaN      NaN  \n",
      "1  15.734000     70.0  \n",
      "2  15.734000     70.0  \n",
      "3  15.734000     70.0  \n",
      "4  16.179001     71.5  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "\n",
      "📁 처리된 파일이 'expanded_economic_indicators.csv'로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = \"filled_economic_indicators.csv\"  # 원본 데이터 파일\n",
    "output_file = \"expanded_economic_indicators.csv\"  # 저장될 파일\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 복사할 경제 지표 리스트\n",
    "economic_indicators = [\"KOSPI\", \"USD/KRW\", \"WTI\", \"VIX\", \"Gold\", \"Silver\", \"MOVE\"]\n",
    "\n",
    "# 수산물 리스트\n",
    "fish_types = [\"광어\", \"농어\", \"대게\", \"방어\", \"연어\", \"우럭\", \"참돔\"]\n",
    "\n",
    "# 새로운 칼럼 생성 (각 경제 지표를 수산물별로 복사)\n",
    "for fish in fish_types:\n",
    "    for indicator in economic_indicators:\n",
    "        df[f\"{fish}_{indicator}\"] = df[indicator]\n",
    "\n",
    "# 처리된 데이터 저장\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 변경된 데이터 확인\n",
    "print(\"\\n✅ 변경된 데이터 (상위 5개 행):\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\n📁 처리된 파일이 '{output_file}'로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 트렌드 데이터 어종별로 그룹화\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 그룹화 함수 생성 및 저장\n",
    "\n",
    "* 입력 파일 : 'nst_{fish}_trend_2025-##-##.csv'\n",
    "* 출력 파일 : '그룹화_nst_{fish}_trend.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_age_groups(file_path, fish_type):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # 연령대 그룹 매핑 딕셔너리 생성\n",
    "    age_mapping = {\n",
    "        '19_24': '20대',\n",
    "        '25_29': '20대',\n",
    "        '30_34': '30대',\n",
    "        '35_39': '30대',\n",
    "        '40_44': '40대',\n",
    "        '45_49': '40대',\n",
    "        '50_54': '50대',\n",
    "        '55_59': '50대',\n",
    "        '60_80': '60대이상'\n",
    "    }\n",
    "    \n",
    "    # 원하는 연령대만 필터링\n",
    "    df = df[df['age'].isin(age_mapping.keys())]\n",
    "    \n",
    "    # 새로운 연령대 컬럼 생성\n",
    "    df['age_group'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # 일자별, 새로운 연령대별로 score 합산\n",
    "    result = df.groupby(['date', 'name', 'age_group'])['score'].sum().reset_index()\n",
    "    \n",
    "    # 피벗 테이블로 변환하여 보기 좋게 정리\n",
    "    pivot_result = result.pivot(index=['date', 'name'], \n",
    "                              columns='age_group', \n",
    "                              values='score').reset_index()\n",
    "    \n",
    "    # 컬럼 순서 정리\n",
    "    column_order = [ 'date', 'name', '20대', '30대', '40대', '50대', '60대이상']\n",
    "    pivot_result = pivot_result[column_order]\n",
    "    \n",
    "    return pivot_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '그룹화_nst_광어_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_농어_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_대게_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_방어_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_연어_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_우럭_trend.csv' 저장 완료!\n",
      "✅ '그룹화_nst_참돔_trend.csv' 저장 완료!\n",
      "\n",
      "📊 최종 처리된 결과 미리보기 (각각 상위 3개 행)\n",
      "\n",
      "광어 데이터 샘플:\n",
      "age_group        date name       20대       30대       40대       50대     60대이상\n",
      "0          2016-01-01   광어  3.716023  3.554474  3.326419  3.809975  1.532954\n",
      "1          2016-01-02   광어  3.827210  3.382407  3.133596  4.049620  1.812825\n",
      "2          2016-01-03   광어  3.682096  3.701303  3.488837  3.279367  1.677356\n",
      "--------------------------------------------------\n",
      "농어 데이터 샘플:\n",
      "age_group        date name       20대       30대       40대       50대     60대이상\n",
      "0          2016-01-01   농어  3.260213  3.581407  3.558843  5.390086  2.163250\n",
      "1          2016-01-02   농어  3.018128  3.318280  3.222967  3.078885  1.804692\n",
      "2          2016-01-03   농어  2.932536  3.082965  2.882074  4.802194  1.134115\n",
      "--------------------------------------------------\n",
      "대게 데이터 샘플:\n",
      "age_group        date name       20대        30대        40대       50대     60대이상\n",
      "0          2016-01-01   대게  9.088761  15.402608  12.426487  9.709127  3.046450\n",
      "1          2016-01-02   대게  7.991422  12.912363   9.378193  7.434733  3.600261\n",
      "2          2016-01-03   대게  7.005608   9.586618   7.596756  5.591602  2.300119\n",
      "--------------------------------------------------\n",
      "방어 데이터 샘플:\n",
      "age_group        date name       20대       30대        40대        50대     60대이상\n",
      "0          2016-01-01   방어  5.906486  7.146976   6.614764  14.358005  3.340613\n",
      "1          2016-01-02   방어  6.485972  9.341901  10.622029  17.453441  7.421370\n",
      "2          2016-01-03   방어  5.147320  5.948592   6.213011   6.207540  2.696208\n",
      "--------------------------------------------------\n",
      "연어 데이터 샘플:\n",
      "age_group        date name        20대        30대        40대        50대  \\\n",
      "0          2016-01-01   연어   4.809541   4.894878   4.750223   5.961751   \n",
      "1          2016-01-02   연어   6.986391   6.831603   5.381617   5.615416   \n",
      "2          2016-01-03   연어  12.569106  16.147838  12.739428  13.177470   \n",
      "\n",
      "age_group     60대이상  \n",
      "0          1.338870  \n",
      "1          3.040053  \n",
      "2          5.182101  \n",
      "--------------------------------------------------\n",
      "우럭 데이터 샘플:\n",
      "age_group        date name       20대       30대       40대       50대     60대이상\n",
      "0          2016-01-01   우럭  3.628428  3.449493  3.129530  4.836146  1.323523\n",
      "1          2016-01-02   우럭  3.745991  3.395083  2.909397  4.233117  1.549985\n",
      "2          2016-01-03   우럭  3.453351  3.871255  3.303071  3.276261  1.962474\n",
      "--------------------------------------------------\n",
      "참돔 데이터 샘플:\n",
      "age_group        date name       20대       30대       40대       50대     60대이상\n",
      "0          2016-01-01   참돔  3.181044  3.557813  3.030270  3.608375  1.308016\n",
      "1          2016-01-02   참돔  3.371687  3.698529  3.199715  3.968064  1.387602\n",
      "2          2016-01-03   참돔  3.387131  3.544599  3.112130  3.432890  1.387602\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  수산물 종류별 파일 경로 및 이름 설정\n",
    "fish_files = {\n",
    "    '광어': 'nst_광어_trend_2025-01-30.csv',\n",
    "    '농어': 'nst_농어_trend_2025-01-30.csv',\n",
    "    '대게': 'nst_대게_trend_2025-01-30.csv',\n",
    "    '방어': 'nst_방어_trend_2025-01-30.csv',\n",
    "    '연어': 'nst_연어_trend_2025-01-30.csv',\n",
    "    '우럭': 'nst_우럭_trend_2025-01-30.csv',\n",
    "    '참돔': 'nst_참돔_trend_2025-01-30.csv'\n",
    "}\n",
    "\n",
    "#  개별 파일 저장 + 결과 리스트로 출력 준비\n",
    "all_results = {}\n",
    "\n",
    "#  모든 파일 처리\n",
    "for fish, file_path in fish_files.items():\n",
    "    try:\n",
    "        processed_df = process_age_groups(file_path, fish)\n",
    "\n",
    "        # 개별 파일 저장\n",
    "        output_filename = f'그룹화_nst_{fish}_trend.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        # 결과 리스트에 추가 (첫 3개 행만 저장)\n",
    "        all_results[fish] = processed_df.head(3)\n",
    "\n",
    "        print(f\"✅ '{output_filename}' 저장 완료!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {fish}: {e}\")\n",
    "\n",
    "#  최종 처리된 결과 출력 (각각 3개 행만 출력)\n",
    "print(\"\\n📊 최종 처리된 결과 미리보기 (각각 상위 3개 행)\\n\")\n",
    "for fish, df_sample in all_results.items():\n",
    "    print(f\"{fish} 데이터 샘플:\")\n",
    "    print(df_sample)\n",
    "    print(\"-\" * 50)  # 구분선 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. merged_trends.csv 저장\n",
    "\n",
    "* 입력 파일 : '그룹화_nst_{fish}_trend.csv'\n",
    "* 출력 파일 : 'merged_trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_fish_trends(file_paths, output_path):\n",
    "    \"\"\"\n",
    "    여러 개의 수산물 트렌드 데이터를 병합하여 날짜별로 정리하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): CSV 파일 경로 리스트.\n",
    "        output_path (str): 병합된 데이터를 저장할 파일 경로.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 병합된 데이터프레임.\n",
    "    \"\"\"\n",
    "    # 첫 번째 파일 로드 및 'name' 컬럼 제거\n",
    "    merged_df = pd.read_csv(file_paths[0], encoding='utf-8')\n",
    "    merged_df = merged_df.drop(columns=['name'], errors='ignore')  # name 컬럼 삭제 (없어도 오류 방지)\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])  # 날짜 형식 변환\n",
    "    \n",
    "    # 파일명에서 수산물 이름 정확히 추출\n",
    "    fish_name = file_paths[0].split('_')[2]  # 파일명 예: '그룹화_nst_광어_trend_2025-01-17.csv' → '광어'\n",
    "    \n",
    "    # 컬럼명 변경 (예: '20대' → '광어_20대')\n",
    "    merged_df = merged_df.rename(columns={col: f\"{fish_name}_{col}\" for col in merged_df.columns if col not in ['date']})\n",
    "\n",
    "    # 나머지 파일들 병합\n",
    "    for file_path in file_paths[1:]:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df = df.drop(columns=['name'], errors='ignore')  # 'name' 컬럼 삭제\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # 파일명에서 수산물 이름 정확히 추출\n",
    "        fish_name = file_path.split('_')[2]  # '그룹화_nst_농어_trend_2025-01-17.csv' → '농어'\n",
    "        \n",
    "        # 컬럼명 변경 (예: '20대' → '농어_20대')\n",
    "        df = df.rename(columns={col: f\"{fish_name}_{col}\" for col in df.columns if col not in ['date']})\n",
    "\n",
    "        # 병합 (중복 컬럼 제거)\n",
    "        merged_df = pd.merge(merged_df, df, on='date', how='outer', suffixes=(None, None))\n",
    "\n",
    "        # 중복 컬럼 자동 제거 (e.g. '광어_20대_x', '광어_20대_y')\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "\n",
    "    # 날짜순 정렬\n",
    "    result = merged_df.sort_values('date')\n",
    "\n",
    "    # CSV 저장\n",
    "    result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n✅ 병합된 데이터가 '{output_path}'에 저장되었습니다!\")\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 병합된 데이터가 'merged_trends.csv'에 저장되었습니다!\n",
      "\n",
      "✅ 병합된 데이터 (상위 5개 행):\n",
      "        date    광어_20대    광어_30대    광어_40대    광어_50대  광어_60대이상    농어_20대  \\\n",
      "0 2016-01-01  3.716023  3.554474  3.326419  3.809975  1.532954  3.260213   \n",
      "1 2016-01-02  3.827210  3.382407  3.133596  4.049620  1.812825  3.018128   \n",
      "2 2016-01-03  3.682096  3.701303  3.488837  3.279367  1.677356  2.932536   \n",
      "3 2016-01-04  3.449136  3.227122  2.811551  3.672470  1.940197  2.730419   \n",
      "4 2016-01-05  3.401118  3.322712  3.168678  2.483900  1.000000  2.676719   \n",
      "\n",
      "     농어_30대    농어_40대    농어_50대  ...    우럭_20대    우럭_30대    우럭_40대    우럭_50대  \\\n",
      "0  3.581407  3.558843  5.390086  ...  3.628428  3.449493  3.129530  4.836146   \n",
      "1  3.318280  3.222967  3.078885  ...  3.745991  3.395083  2.909397  4.233117   \n",
      "2  3.082965  2.882074  4.802194  ...  3.453351  3.871255  3.303071  3.276261   \n",
      "3  3.361940  3.029135  3.059708  ...  3.333151  3.069274  3.539530  4.207426   \n",
      "4  3.040365  3.129464  3.153394  ...  3.304805  3.384335  2.707205  2.638036   \n",
      "\n",
      "   우럭_60대이상    참돔_20대    참돔_30대    참돔_40대    참돔_50대  참돔_60대이상  \n",
      "0  1.323523  3.181044  3.557813  3.030270  3.608375  1.308016  \n",
      "1  1.549985  3.371687  3.698529  3.199715  3.968064  1.387602  \n",
      "2  1.962474  3.387131  3.544599  3.112130  3.432890  1.387602  \n",
      "3  1.637661  2.941569  2.955552  2.781371  3.147489  1.296743  \n",
      "4  1.000000  2.821804  3.201582  2.908575  2.748799  1.279332  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시:\n",
    "files = [\n",
    "    '그룹화_nst_광어_trend.csv', \n",
    "    '그룹화_nst_농어_trend.csv', \n",
    "    '그룹화_nst_대게_trend.csv', \n",
    "    '그룹화_nst_방어_trend.csv', \n",
    "    '그룹화_nst_연어_trend.csv', \n",
    "    '그룹화_nst_우럭_trend.csv', \n",
    "    '그룹화_nst_참돔_trend.csv'\n",
    "]\n",
    "\n",
    "# 병합 실행\n",
    "merged_result = merge_fish_trends(files, 'merged_trends.csv')\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n✅ 병합된 데이터 (상위 5개 행):\")\n",
    "print(merged_result.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 기상 데이터 전처리\n",
    "\n",
    "* 입력 파일 : 'forecast_agg.csv'\n",
    "* 출력 파일 : 'weatherdata_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "\n",
    "def create_weather_columns(df, station_cols, output_path):\n",
    "   result_df = df.copy()\n",
    "   result_df['일시'] = pd.to_datetime(result_df['일시'])\n",
    "   \n",
    "   final_df = pd.DataFrame({'일시': result_df['일시'].unique()})\n",
    "   \n",
    "   for station, columns in station_cols.items():\n",
    "       station_data = result_df[result_df['지점'] == station].copy()\n",
    "       \n",
    "       for orig_col, new_col in columns:\n",
    "           station_col = station_data[['일시', orig_col]].copy()\n",
    "           final_df = pd.merge(final_df, station_col.rename(columns={orig_col: new_col}), \n",
    "                             on='일시', how='left')\n",
    "   \n",
    "   # 칼럼을 가나다순으로 정렬 ('일시' 컬럼은 첫번째로 유지)\n",
    "   sorted_cols = ['일시'] + sorted([col for col in final_df.columns if col != '일시'])\n",
    "   result = final_df[sorted_cols].sort_values('일시')\n",
    "   \n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('forecast_agg.csv', encoding='utf-8')\n",
    "\n",
    "# 피쳐 선택\n",
    "station_columns = {\n",
    "\t22105: [\n",
    "\t\t('기온', '광어_기온_22105'),\n",
    "\t\t('기온', '농어_기온_22105'),\n",
    "\t\t('기온', '대게_기온_22105'),\n",
    "\t\t('파주기', '대게_파주기_22105'),\n",
    "\t\t('파주기', '방어_파주기_22105'),\n",
    "\t\t('기온', '연어_기온_22105'),\n",
    "\t\t('습도', '연어_습도_22105')\n",
    "\t],\n",
    "    \n",
    "\t22107: [\n",
    "\t\t('수온', '광어_수온_22107'),\n",
    "\t\t('수온', '농어_수온_22107'),\n",
    "\t\t('수온', '방어_수온_22107'),\n",
    "\t\t('수온', '연어_수온_22107'),\n",
    "\t\t('수온', '참돔_수온_22107')\n",
    "\t],\n",
    "\n",
    "\t22186: [\n",
    "\t\t('습도', '광어_습도_22186'),\n",
    "\t\t('습도', '농어_습도_22186'),\n",
    "\t\t('기온', '우럭_기온_22186'),\n",
    "\t\t('수온', '우럭_수온_22186')\n",
    "\t\t],\n",
    "        \n",
    "\t22188: [\n",
    "\t\t('수온', '대게_수온_22188'),\n",
    "\t\t('습도', '대게_습도_22188')\n",
    "\t\t],        \n",
    "\n",
    "\t22189: [\n",
    "\t\t('파주기', '우럭_파주기_22189')\n",
    "\t\t],       \n",
    "\n",
    "\t22190: [\n",
    "\t\t('파주기', '광어_파주기_22190'),\n",
    "        ('파주기', '농어_파주기_22190'),\n",
    "        ('기온', '방어_기온_22190'),\n",
    "        ('습도', '방어_습도_22190'),\n",
    "        ('파주기', '연어_파주기_22190'),\n",
    "        ('습도', '우럭_습도_22190'),\n",
    "        ('기온', '참돔_기온_22190'),\n",
    "        ('습도', '참돔_습도_22190'),\n",
    "        ('파주기', '참돔_파주기_22190')\n",
    "\t\t]  \n",
    "\n",
    "\t}\n",
    "\n",
    "result = create_weather_columns(df, station_columns, 'weatherdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 변수 하나로 합치기\n",
    "\n",
    "* 입력 파일 : 'expanded_economic_indicators_.csv',  'merged_trends.csv', 'weatherdata_processed.csv'\n",
    "* 출력 파일 : 'merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(output_path='merged_all_data.csv'):\n",
    "   # 각 파일 로드 및 전처리\n",
    "   \n",
    "   economic = pd.read_csv('expanded_economic_indicators.csv', encoding='utf-8')\n",
    "   economic['Date'] = pd.to_datetime(economic['Date'])\n",
    "   economic = economic.rename(columns={'Date': '날짜'})\n",
    "   \n",
    "   trends = pd.read_csv('merged_trends.csv', encoding='utf-8')\n",
    "   trends['date'] = pd.to_datetime(trends['date'])\n",
    "   trends = trends.rename(columns={'date': '날짜'})\n",
    "   \n",
    "   weather = pd.read_csv('weatherdata_processed.csv', encoding='utf-8')\n",
    "   weather['일시'] = pd.to_datetime(weather['일시'])\n",
    "   weather = weather.rename(columns={'일시': '날짜'})\n",
    "   \n",
    "   # 모든 날짜 추출\n",
    "   all_dates = pd.concat([\n",
    "        economic['날짜'], \n",
    "        trends['날짜'], \n",
    "       weather['날짜']\n",
    "   ]).unique()\n",
    "   \n",
    "   # 날짜 기준 데이터프레임 생성\n",
    "   date_df = pd.DataFrame({'날짜': all_dates})\n",
    "   date_df = date_df.sort_values('날짜')\n",
    "   \n",
    "   # 데이터 병합\n",
    "   dfs = [\n",
    "\t\tdate_df, \n",
    "\t\teconomic, \n",
    "\t\ttrends,\n",
    "\t\tweather\n",
    "   ]\n",
    "   \n",
    "   result = dfs[0]\n",
    "   for df in dfs[1:]:\n",
    "       result = pd.merge(result, df, on='날짜', how='left')\n",
    "   \n",
    "   # 결과 저장\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = merge_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 결측치 채우기\n",
    "\n",
    "* 입력 파일 : 'merged_all_data.csv'\n",
    "* 출력 파일 : 'filled_merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_values(file_path, output_path):\n",
    "    \"\"\"\n",
    "    CSV 파일에서 결측치를 `ffill`을 사용하여 채우되, 이전 값이 없으면 그대로 둠.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 입력 CSV 파일 경로.\n",
    "        output_path (str): 처리된 데이터를 저장할 경로.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 결측치가 채워진 데이터프레임.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV 파일 로드\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 결측치 확인 (처리 전)\n",
    "    missing_before = df.isnull().sum()\n",
    "\n",
    "    # 결측치 `ffill`로 채우기 (이전 값이 없으면 그대로 둠)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # 결측치 확인 (처리 후)\n",
    "    missing_after = df.isnull().sum()\n",
    "\n",
    "    # 변경된 결측치 개수 출력\n",
    "    missing_summary = pd.DataFrame({'Before': missing_before, 'After': missing_after})\n",
    "    print(\"\\n🔍 결측치 처리 전후 비교:\")\n",
    "    print(missing_summary)\n",
    "\n",
    "    # 처리된 데이터 저장\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ 처리된 데이터가 '{output_path}'에 저장되었습니다.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1915651589.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 결측치 처리 전후 비교:\n",
      "              Before  After\n",
      "날짜                 0      0\n",
      "USD/KRW            0      0\n",
      "KOSPI              1      1\n",
      "WTI                1      1\n",
      "VIX                1      1\n",
      "...              ...    ...\n",
      "우럭_파주기_22189     494    355\n",
      "참돔_기온_22190      459    342\n",
      "참돔_수온_22107      210      0\n",
      "참돔_습도_22190      461    342\n",
      "참돔_파주기_22190     421    342\n",
      "\n",
      "[120 rows x 2 columns]\n",
      "\n",
      "✅ 처리된 데이터가 'filled_merged_all_data.csv'에 저장되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>USD/KRW</th>\n",
       "      <th>KOSPI</th>\n",
       "      <th>WTI</th>\n",
       "      <th>VIX</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>MOVE</th>\n",
       "      <th>광어_KOSPI</th>\n",
       "      <th>광어_USD/KRW</th>\n",
       "      <th>...</th>\n",
       "      <th>연어_습도_22105</th>\n",
       "      <th>연어_파주기_22190</th>\n",
       "      <th>우럭_기온_22186</th>\n",
       "      <th>우럭_수온_22186</th>\n",
       "      <th>우럭_습도_22190</th>\n",
       "      <th>우럭_파주기_22189</th>\n",
       "      <th>참돔_기온_22190</th>\n",
       "      <th>참돔_수온_22107</th>\n",
       "      <th>참돔_습도_22190</th>\n",
       "      <th>참돔_파주기_22190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1092.699951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1092.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>65.130435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.382609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>60.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>39.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>52.689999</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>15.734000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1926.439941</td>\n",
       "      <td>1093.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.475000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>1915.750000</td>\n",
       "      <td>50.040001</td>\n",
       "      <td>19.920000</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>16.179001</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1915.750000</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>57.086957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.475000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>2025-01-25</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>2777.300049</td>\n",
       "      <td>31.023001</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>2025-01-26</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>74.660004</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>2777.300049</td>\n",
       "      <td>31.023001</td>\n",
       "      <td>86.750000</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1433.930054</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>2025-01-27</td>\n",
       "      <td>1427.630005</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>73.169998</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>2737.500000</td>\n",
       "      <td>30.254000</td>\n",
       "      <td>95.050003</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1427.630005</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>2025-01-28</td>\n",
       "      <td>1417.920044</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>73.769997</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>2766.800049</td>\n",
       "      <td>30.726999</td>\n",
       "      <td>92.830002</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1417.920044</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>2025-01-29</td>\n",
       "      <td>1443.260010</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>72.620003</td>\n",
       "      <td>16.559999</td>\n",
       "      <td>2769.100098</td>\n",
       "      <td>31.238001</td>\n",
       "      <td>90.730003</td>\n",
       "      <td>2536.800049</td>\n",
       "      <td>1443.260010</td>\n",
       "      <td>...</td>\n",
       "      <td>53.545455</td>\n",
       "      <td>6.004167</td>\n",
       "      <td>3.2375</td>\n",
       "      <td>9.683333</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>5.3375</td>\n",
       "      <td>5.825</td>\n",
       "      <td>16.263636</td>\n",
       "      <td>59.458333</td>\n",
       "      <td>6.004167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3682 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              날짜      USD/KRW        KOSPI        WTI        VIX         Gold  \\\n",
       "0     2015-01-01  1092.699951          NaN        NaN        NaN          NaN   \n",
       "1     2015-01-02  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "2     2015-01-03  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "3     2015-01-04  1093.599976  1926.439941  52.689999  17.790001  1186.000000   \n",
       "4     2015-01-05  1111.000000  1915.750000  50.040001  19.920000  1203.900024   \n",
       "...          ...          ...          ...        ...        ...          ...   \n",
       "3677  2025-01-25  1433.930054  2536.800049  74.660004  14.850000  2777.300049   \n",
       "3678  2025-01-26  1433.930054  2536.800049  74.660004  14.850000  2777.300049   \n",
       "3679  2025-01-27  1427.630005  2536.800049  73.169998  17.900000  2737.500000   \n",
       "3680  2025-01-28  1417.920044  2536.800049  73.769997  16.410000  2766.800049   \n",
       "3681  2025-01-29  1443.260010  2536.800049  72.620003  16.559999  2769.100098   \n",
       "\n",
       "         Silver       MOVE     광어_KOSPI   광어_USD/KRW  ...  연어_습도_22105  \\\n",
       "0           NaN        NaN          NaN  1092.699951  ...    65.130435   \n",
       "1     15.734000  70.000000  1926.439941  1093.599976  ...    60.083333   \n",
       "2     15.734000  70.000000  1926.439941  1093.599976  ...    39.750000   \n",
       "3     15.734000  70.000000  1926.439941  1093.599976  ...    54.000000   \n",
       "4     16.179001  71.500000  1915.750000  1111.000000  ...    57.086957   \n",
       "...         ...        ...          ...          ...  ...          ...   \n",
       "3677  31.023001  86.750000  2536.800049  1433.930054  ...    53.545455   \n",
       "3678  31.023001  86.750000  2536.800049  1433.930054  ...    53.545455   \n",
       "3679  30.254000  95.050003  2536.800049  1427.630005  ...    53.545455   \n",
       "3680  30.726999  92.830002  2536.800049  1417.920044  ...    53.545455   \n",
       "3681  31.238001  90.730003  2536.800049  1443.260010  ...    53.545455   \n",
       "\n",
       "      연어_파주기_22190  우럭_기온_22186  우럭_수온_22186  우럭_습도_22190  우럭_파주기_22189  \\\n",
       "0              NaN          NaN          NaN          NaN           NaN   \n",
       "1              NaN          NaN          NaN          NaN           NaN   \n",
       "2              NaN          NaN          NaN          NaN           NaN   \n",
       "3              NaN          NaN          NaN          NaN           NaN   \n",
       "4              NaN          NaN          NaN          NaN           NaN   \n",
       "...            ...          ...          ...          ...           ...   \n",
       "3677      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3678      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3679      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3680      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "3681      6.004167       3.2375     9.683333    59.458333        5.3375   \n",
       "\n",
       "      참돔_기온_22190  참돔_수온_22107  참돔_습도_22190  참돔_파주기_22190  \n",
       "0             NaN    16.382609          NaN           NaN  \n",
       "1             NaN    16.375000          NaN           NaN  \n",
       "2             NaN    16.283333          NaN           NaN  \n",
       "3             NaN    16.475000          NaN           NaN  \n",
       "4             NaN    16.475000          NaN           NaN  \n",
       "...           ...          ...          ...           ...  \n",
       "3677        5.825    16.263636    59.458333      6.004167  \n",
       "3678        5.825    16.263636    59.458333      6.004167  \n",
       "3679        5.825    16.263636    59.458333      6.004167  \n",
       "3680        5.825    16.263636    59.458333      6.004167  \n",
       "3681        5.825    16.263636    59.458333      6.004167  \n",
       "\n",
       "[3682 rows x 120 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'merged_all_data.csv'\n",
    "output_path = 'filled_merged_all_data.csv'\n",
    "\n",
    "fill_missing_values(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 타임래그 적용하기 \n",
    "\n",
    "* 입력 파일 : 'filled_merged_all_data.csv'\n",
    "* 출력 파일 : 'timelagged_features.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. 설정된 타임래그 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_39148\\1547269827.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_{lag}'] = df[col].shift(lag)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('filled_merged_all_data.csv', encoding='utf-8')\n",
    "df['date'] = pd.to_datetime(df['날짜'])\n",
    "\n",
    "# 각 칼럼별 lag 일수 정의\n",
    "lag_days = {\n",
    "\n",
    "'광어_KOSPI' : 136,\n",
    "'광어_USD/KRW' : 1,\n",
    "'광어_WTI' : 1,\n",
    "'광어_VIX' : 399,\n",
    "'광어_Gold' : 314,\n",
    "'광어_Silver' : 238,\n",
    "'광어_MOVE' : 18,\n",
    "\n",
    "'농어_KOSPI' : 179,\n",
    "'농어_USD/KRW' : 1,\n",
    "'농어_WTI' : 100,\n",
    "'농어_VIX' : 391,\n",
    "'농어_Gold' : 361,\n",
    "'농어_Silver' : 290,\n",
    "'농어_MOVE' : 1,\n",
    "\n",
    "'대게_KOSPI' : 148,\n",
    "'대게_USD/KRW' : 90,\n",
    "'대게_WTI' : 91,\n",
    "# '대게_VIX' : \n",
    "'대게_Gold' : 177,\n",
    "'대게_Silver' : 177,\n",
    "# '대게_MOVE' : \n",
    "\n",
    "'방어_KOSPI' : 282,\n",
    "'방어_USD/KRW' : 387,\n",
    "'방어_WTI' : 399,\n",
    "'방어_VIX' : 133,\n",
    "'방어_Gold' : 1,\n",
    "'방어_Silver' : 1,\n",
    "'방어_MOVE' : 381,\n",
    "\n",
    "'연어_KOSPI' : 329,\n",
    "'연어_USD/KRW' : 1,\n",
    "'연어_WTI' : 199,\n",
    "'연어_VIX' : 399,\n",
    "'연어_Gold' : 1,\n",
    "'연어_Silver' : 399,\n",
    "'연어_MOVE' : 71,\n",
    "\n",
    "'우럭_KOSPI' : 155,\n",
    "'우럭_USD/KRW' : 121,\n",
    "'우럭_WTI' : 14,\n",
    "'우럭_VIX' : 38,\n",
    "'우럭_Gold' : 146,\n",
    "'우럭_Silver' : 125,\n",
    "# '우럭_MOVE' : \n",
    "\n",
    "'참돔_KOSPI' : 399,\n",
    "'참돔_USD/KRW' : 11,\n",
    "'참돔_WTI' : 150,\n",
    "# '참돔_VIX' : \n",
    "# '참돔_Gold' : \n",
    "# '참돔_Silver' : \n",
    "'참돔_MOVE' : 201,\n",
    " \n",
    "'광어_20대' : 250,\n",
    "'광어_30대' : 317,\n",
    "'광어_40대' : 330,\n",
    "'광어_50대' : 395,\n",
    "'광어_60대이상' : 339,\n",
    "\n",
    "'농어_20대' : 305,\n",
    "'농어_30대' : 307,\n",
    "'농어_40대' : 309,\n",
    "'농어_50대' : 309,\n",
    "'농어_60대이상' : 273,\n",
    "\n",
    "'대게_20대' : 273,\n",
    "'대게_30대' : 370,\n",
    "'대게_40대' : 5,\n",
    "'대게_50대' : 5,\n",
    "'대게_60대이상' : 5,\n",
    "\n",
    "'방어_20대' : 22,\n",
    "'방어_30대' : 26,\n",
    "'방어_40대' : 256,\n",
    "'방어_50대' : 256,\n",
    "'방어_60대이상' : 26,\n",
    "\n",
    "'연어_20대' : 20,\n",
    "'연어_30대' : 15,\n",
    "'연어_40대' : 21,\n",
    "'연어_50대' : 15,\n",
    "'연어_60대이상' : 397,\n",
    "\n",
    "'우럭_20대' : 22,\n",
    "'우럭_30대' : 354,\n",
    "'우럭_40대' : 354,\n",
    "'우럭_50대' : 220,\n",
    "'우럭_60대이상' : 219,\n",
    "\n",
    "'참돔_20대' : 197,\n",
    "'참돔_30대' : 167,\n",
    "'참돔_40대' : 278,\n",
    "'참돔_50대' : 193,\n",
    "'참돔_60대이상' : 14,\n",
    "\n",
    "\n",
    "'광어_기온_22105' : 97,\n",
    "'광어_수온_22107' : 79,\n",
    "'광어_습도_22186' : 349,\n",
    "'광어_파주기_22190' : 103,\n",
    "\n",
    "'농어_기온_22105' : 103,\n",
    "'농어_수온_22107' : 81,\n",
    "'농어_습도_22186' : 333,\n",
    "'농어_파주기_22190' : 115,\n",
    "\n",
    "'대게_기온_22105' : 150,\n",
    "'대게_수온_22188' : 140,\n",
    "'대게_습도_22188' : 355,\n",
    "'대게_파주기_22105' : 174,\n",
    "\n",
    "'방어_기온_22190' : 114,\n",
    "'방어_수온_22107' : 118,\n",
    "'방어_습도_22190' : 158,\n",
    "'방어_파주기_22105' : 191,\n",
    "\n",
    "'연어_기온_22105' : 394,\n",
    "'연어_수온_22107' : 341,\n",
    "'연어_습도_22105' : 8,\n",
    "'연어_파주기_22190' : 9,\n",
    "\n",
    "'우럭_기온_22186' : 118,\n",
    "'우럭_수온_22186' : 113,\n",
    "'우럭_습도_22190' : 159,\n",
    "'우럭_파주기_22189' : 172,\n",
    "\n",
    "'참돔_기온_22190' : 91,\n",
    "'참돔_수온_22107' : 72,\n",
    "'참돔_습도_22190' : 115,\n",
    "'참돔_파주기_22190' : 107\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# 각 칼럼에 대해 지정된 lag 적용\n",
    "for col, lag in lag_days.items():\n",
    "   if col in df.columns:\n",
    "       df[f'{col}_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv('timelagged_features.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2. lag_1 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('filled_merged_all_data.csv', encoding='utf-8')\n",
    "df['date'] = pd.to_datetime(df['날짜'])\n",
    "\n",
    "# 각 칼럼별 lag 일수 정의\n",
    "lag_days = {\n",
    "\n",
    " \n",
    "'광어_20대' : 1,\n",
    "'광어_30대' : 1,\n",
    "'광어_40대' : 1,\n",
    "'광어_50대' : 1,\n",
    "'광어_60대이상' : 1,\n",
    "\n",
    "'농어_20대' : 1,\n",
    "'농어_30대' : 1,\n",
    "'농어_40대' : 1,\n",
    "'농어_50대' : 1,\n",
    "'농어_60대이상' : 1,\n",
    "\n",
    "'대게_20대' : 1,\n",
    "'대게_30대' : 1,\n",
    "'대게_40대' : 1,\n",
    "'대게_50대' : 1,\n",
    "'대게_60대이상' : 1,\n",
    "\n",
    "'방어_20대' : 1,\n",
    "'방어_30대' : 1,\n",
    "'방어_40대' : 1,\n",
    "'방어_50대' : 1,\n",
    "'방어_60대이상' : 1,\n",
    "\n",
    "'연어_20대' : 1,\n",
    "'연어_30대' : 1,\n",
    "'연어_40대' : 1,\n",
    "'연어_50대' : 1,\n",
    "'연어_60대이상' : 1,\n",
    "\n",
    "'우럭_20대' : 1,\n",
    "'우럭_30대' : 1,\n",
    "'우럭_40대' : 1,\n",
    "'우럭_50대' : 1,\n",
    "'우럭_60대이상' : 1,\n",
    "\n",
    "'참돔_20대' : 1,\n",
    "'참돔_30대' : 1,\n",
    "'참돔_40대' : 1,\n",
    "'참돔_50대' : 1,\n",
    "'참돔_60대이상' : 1\n",
    "}\n",
    "\n",
    "# 각 칼럼에 대해 지정된 lag 적용\n",
    "for col, lag in lag_days.items():\n",
    "   if col in df.columns:\n",
    "       df[f'{col}_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv('timelagged_features1.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3. 트렌드 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 변경된 데이터 (상위 5개 행):\n",
      "         date  광어_20대_1  광어_20대_250  광어_30대_1  광어_30대_317  광어_40대_1  \\\n",
      "0  2015-01-01       NaN         NaN       NaN         NaN       NaN   \n",
      "1  2015-01-02       NaN         NaN       NaN         NaN       NaN   \n",
      "2  2015-01-03       NaN         NaN       NaN         NaN       NaN   \n",
      "3  2015-01-04       NaN         NaN       NaN         NaN       NaN   \n",
      "4  2015-01-05       NaN         NaN       NaN         NaN       NaN   \n",
      "\n",
      "   광어_40대_330  광어_50대_1  광어_50대_395  광어_60대이상_1  ...  참돔_60대이상_1  참돔_60대이상_14  \\\n",
      "0         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "1         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "2         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "3         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "4         NaN       NaN         NaN         NaN  ...         NaN          NaN   \n",
      "\n",
      "   참돔_KOSPI_399  참돔_MOVE_201  참돔_USD/KRW_11  참돔_WTI_150  참돔_기온_22190_91  \\\n",
      "0           NaN          NaN            NaN         NaN             NaN   \n",
      "1           NaN          NaN            NaN         NaN             NaN   \n",
      "2           NaN          NaN            NaN         NaN             NaN   \n",
      "3           NaN          NaN            NaN         NaN             NaN   \n",
      "4           NaN          NaN            NaN         NaN             NaN   \n",
      "\n",
      "   참돔_수온_22107_72  참돔_습도_22190_115  참돔_파주기_22190_107  \n",
      "0             NaN              NaN               NaN  \n",
      "1             NaN              NaN               NaN  \n",
      "2             NaN              NaN               NaN  \n",
      "3             NaN              NaN               NaN  \n",
      "4             NaN              NaN               NaN  \n",
      "\n",
      "[5 rows x 142 columns]\n",
      "\n",
      "📁 처리된 파일이 'final_timelagged_features.csv'로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path1 = \"timelagged_features.csv\"\n",
    "file_path2 = \"timelagged_features1.csv\"\n",
    "output_file = \"final_timelagged_features.csv\"  # 저장될 파일\n",
    "\n",
    "# 데이터 로드\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# 공통 칼럼 확인\n",
    "common_columns = set(df1.columns) & set(df2.columns)\n",
    "\n",
    "# 중복되지 않는 칼럼만 선택\n",
    "unique_columns_df1 = set(df1.columns) - common_columns\n",
    "unique_columns_df2 = set(df2.columns) - common_columns\n",
    "\n",
    "# 'date' 칼럼 포함하여 새로운 데이터프레임 생성\n",
    "final_columns = ['date'] + list(unique_columns_df1) + list(unique_columns_df2)\n",
    "df_final = pd.concat([df1[['date']], df1[list(unique_columns_df1)], df2[list(unique_columns_df2)]], axis=1)\n",
    "\n",
    "# 'date' 칼럼을 제외한 나머지 칼럼을 가나다 순으로 정렬\n",
    "sorted_columns = sorted([col for col in df_final.columns if col != 'date'])\n",
    "\n",
    "# 'date' 칼럼을 앞에 두고 정렬된 칼럼 배치\n",
    "df_final = df_final[['date'] + sorted_columns]\n",
    "\n",
    "# 결과 저장\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "# 변경된 데이터 확인\n",
    "print(\"\\n✅ 변경된 데이터 (상위 5개 행):\")\n",
    "print(df_final.head())\n",
    "\n",
    "print(f\"\\n📁 처리된 파일이 '{output_file}'로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4. 어종별 타임래그 데이터 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 광어_timelagged_features.csv with 22 columns\n",
      "Created 농어_timelagged_features.csv with 22 columns\n",
      "Created 대게_timelagged_features.csv with 20 columns\n",
      "Created 방어_timelagged_features.csv with 22 columns\n",
      "Created 연어_timelagged_features.csv with 22 columns\n",
      "Created 우럭_timelagged_features.csv with 21 columns\n",
      "Created 참돔_timelagged_features.csv with 19 columns\n"
     ]
    }
   ],
   "source": [
    "# 어종별 분류 \n",
    "\n",
    "def split_by_fish():\n",
    "    df = pd.read_csv('final_timelagged_features.csv', encoding='utf-8')\n",
    "    fish_types = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "    \n",
    "    for fish in fish_types:\n",
    "        # Get date and fish-specific columns\n",
    "        fish_cols = ['date'] + [col for col in df.columns if fish in col]\n",
    "        fish_df = df[fish_cols]\n",
    "        \n",
    "        output_file = f'{fish}_timelagged_features.csv'\n",
    "        fish_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f'Created {output_file} with {len(fish_cols)} columns')\n",
    "\n",
    "split_by_fish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 가격 데이터 \n",
    "\n",
    "* 입력 파일 : 'ikh_item_price_2025-01-30.csv\n",
    "* 출력 파일 : 'item_price_lag.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-1. 노량진 2층 버리기\n",
    "\n",
    "* 입력 파일 : 'ikh_item_price_2025-01-30.csv\n",
    "* 출력 파일 : 'item_price_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 변경된 데이터 (상위 5개 행):\n",
      "    priceDate  minPrice  avgPrice  maxPrice item market\n",
      "0  2015-03-06     20000     20000     20000   농어  노량진시장\n",
      "1  2015-03-14     25000     25000     25000   농어  노량진시장\n",
      "2  2015-04-18     25000     27500     30000   농어  노량진시장\n",
      "3  2015-04-28     25000     25000     25000   농어  노량진시장\n",
      "4  2015-05-18     25000     25000     25000   농어  노량진시장\n",
      "\n",
      "📁 처리된 파일이 'item_price_deleted.csv'로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = \"ikh_item_price_2025-01-30.csv\"  # 원본 데이터 파일\n",
    "output_file = \"item_price_deleted.csv\"  # 저장될 파일\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 'market' 칼럼에서 '노량진 2층'인 행 제거\n",
    "df = df[df['market'] != '노량진 2층']\n",
    "\n",
    "# '노량진 1층'을 '노량진시장'으로 변경\n",
    "df['market'] = df['market'].replace('노량진 1층', '노량진시장')\n",
    "\n",
    "# 처리된 데이터 저장\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 변경된 데이터 확인\n",
    "print(\"\\n✅ 변경된 데이터 (상위 5개 행):\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\n📁 처리된 파일이 '{output_file}'로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-2. 모든 날짜 생성 및 결측치 채우기\n",
    "\n",
    "* 입력 파일 : 'item_price_deleted.csv\n",
    "* 출력 파일 : 'item_price_filled_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 item_price_filled_deleted.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 파일 로드\n",
    "file_path = \"item_price_deleted.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 날짜 데이터 변환\n",
    "df[\"priceDate\"] = pd.to_datetime(df[\"priceDate\"])\n",
    "\n",
    "# item과 market의 조합별로 처리\n",
    "filled_dfs = []\n",
    "\n",
    "for (item, market), group in df.groupby([\"item\", \"market\"]):\n",
    "    # 전체 날짜 범위 생성\n",
    "    full_date_range = pd.date_range(start=group[\"priceDate\"].min(), end=group[\"priceDate\"].max())\n",
    "\n",
    "    # 기존 데이터프레임을 날짜를 기준으로 리인덱싱\n",
    "    group = group.set_index(\"priceDate\").reindex(full_date_range).reset_index()\n",
    "    group.rename(columns={\"index\": \"priceDate\"}, inplace=True)\n",
    "\n",
    "    # item과 market 정보 채우기\n",
    "    group[\"item\"] = item\n",
    "    group[\"market\"] = market\n",
    "\n",
    "    # 결측값을 앞의 값으로 채우기 (ffill)\n",
    "    group.ffill(inplace=True)\n",
    "\n",
    "    # 리스트에 추가\n",
    "    filled_dfs.append(group)\n",
    "\n",
    "# 모든 데이터를 합침\n",
    "filled_df = pd.concat(filled_dfs, ignore_index=True)\n",
    "\n",
    "# 결과 저장 (원하는 경로로 변경 가능)\n",
    "output_path = \"item_price_filled_deleted.csv\"\n",
    "filled_df.to_csv(output_path, index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"데이터가 {output_path} 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-3. 1일 타임래그 생성\n",
    "\n",
    "* 입력 파일 : 'item_price_filled_deleted.csv\n",
    "* 출력 파일 : 'item_price_lag_filled_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드 (파일 경로는 적절하게 수정)\n",
    "file_path = \"item_price_filled_deleted.csv\"\n",
    "output_file = \"item_price_lag_filled_deleted.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 날짜 칼럼 이름을 'priceDate'에서 datetime 형식으로 변환\n",
    "df['priceDate'] = pd.to_datetime(df['priceDate'])\n",
    "\n",
    "# avgPrice에 1일 타임래그 적용\n",
    "df['avgPrice_lag_1'] = df['avgPrice'].shift(1)\n",
    "\n",
    "# 처리된 데이터 저장\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 시장 데이터 원핫 인코딩\n",
    "\n",
    "* 입력 파일 : 'item_price_lag_filled_deleted.csv'\n",
    "* 출력 파일 : 'item_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import DateTime\n",
    "\n",
    "def transform_market_data(file_path):\n",
    "    # 데이터 읽기\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 시장별 더미변수 생성 (0, 1로 인코딩)\n",
    "    market_dummies = pd.get_dummies(df['market'], prefix='m').astype(int)\n",
    "    \n",
    "     # priceDate 칼럼명을 date로 변경\n",
    "    df = df.rename(columns={'priceDate': 'date'})\n",
    "    \n",
    "    # 원본 데이터와 더미변수 결합\n",
    "    result = pd.concat([\n",
    "        df[['date', 'item']],  # 'priceDate' → 'date'로 변경됨\n",
    "        market_dummies,\n",
    "        df[['avgPrice', 'avgPrice_lag_1']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # 날짜와 어종으로 정렬\n",
    "    result = result.sort_values(['date', 'item'])\n",
    "    \n",
    "    # 변환된 데이터 저장\n",
    "    output_file = 'item_price_oneHot.csv'\n",
    "    result.to_csv(output_file, index=False)\n",
    "    print(f\"✅ 생성된 파일: {output_file}\")\n",
    "    print(\"\\n🔍 처음 10개 컬럼:\", result.columns[:10].tolist())\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 생성된 파일: item_price_oneHot.csv\n",
      "\n",
      "🔍 처음 10개 컬럼: ['date', 'item', 'm_가락시장', 'm_강서농수산물시장', 'm_구리농수산물시장', 'm_노량진시장', 'm_마포농수산물시장', 'm_부산민락어민활어직판장', 'm_소래포구종합어시장', 'm_수원농수산물시장']\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "df = transform_market_data('item_price_lag_filled_deleted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 가격데이터 어종별로 나누기\n",
    "\n",
    "* 입력 파일 : 'item_price_oneHot.csv'\n",
    "* 출력 파일 : '{fish}_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_market_data():\n",
    "    df = pd.read_csv('item_price_oneHot.csv')\n",
    "    \n",
    "    for fish in df['item'].unique():\n",
    "        fish_df = df[df['item'] == fish]\n",
    "        output_file = f'{fish}_price_oneHot.csv'\n",
    "      # fish_df = fish_df.drop('item', axis=1)\n",
    "        fish_df.to_csv(output_file, index=False)\n",
    "        print(f'{fish} 데이터 생성 완료: {len(fish_df)}개 행')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대게 데이터 생성 완료: 24131개 행\n",
      "광어 데이터 생성 완료: 29489개 행\n",
      "농어 데이터 생성 완료: 28543개 행\n",
      "연어 데이터 생성 완료: 27468개 행\n",
      "참돔 데이터 생성 완료: 25041개 행\n",
      "방어 데이터 생성 완료: 9715개 행\n",
      "우럭 데이터 생성 완료: 5641개 행\n"
     ]
    }
   ],
   "source": [
    "split_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 최종 데이터 합치기\n",
    "\n",
    "* 입력 파일 : '{fish}_price_oneHot.csv', '{fish}_timelagged_features.csv'\n",
    "* 출력 파일 : '{fish}_price_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_price(fish):\n",
    "    # 두 데이터셋 로드\n",
    "    features_df = pd.read_csv(f'{fish}_timelagged_features.csv')\n",
    "    price_df = pd.read_csv(f'{fish}_price_oneHot.csv')\n",
    "    \n",
    "    # 날짜 컬럼명 통일\n",
    "    price_df = price_df.rename(columns={'dmdpriceDate': 'date'})\n",
    "    \n",
    "    # 날짜 기준으로 데이터 병합\n",
    "    merged_df = pd.merge(price_df, features_df, on='date', how='left')\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    output_file = f'{fish}_price_features.csv'\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f'생성된 파일: {output_file}')\n",
    "    print(f'전체 컬럼 수: {len(merged_df.columns)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 파일: 광어_price_features.csv\n",
      "전체 컬럼 수: 35\n",
      "생성된 파일: 농어_price_features.csv\n",
      "전체 컬럼 수: 35\n",
      "생성된 파일: 대게_price_features.csv\n",
      "전체 컬럼 수: 33\n",
      "생성된 파일: 방어_price_features.csv\n",
      "전체 컬럼 수: 35\n",
      "생성된 파일: 연어_price_features.csv\n",
      "전체 컬럼 수: 35\n",
      "생성된 파일: 우럭_price_features.csv\n",
      "전체 컬럼 수: 34\n",
      "생성된 파일: 참돔_price_features.csv\n",
      "전체 컬럼 수: 32\n"
     ]
    }
   ],
   "source": [
    "fish_list = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "\n",
    "for fish in fish_list:\n",
    "    merge_features_price(fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 최종 결측치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-1. 결측치 없는 행만 남기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_fish_data(fish_list, input_path, output_path):\n",
    "    \"\"\"\n",
    "    여러 수산물 데이터에서 NULL 값이 없는 첫 번째 행부터 마지막 행까지 필터링하여 저장하는 함수.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): 수산물 목록 (예: ['광어', '농어', ...])\n",
    "        input_path (str): 원본 데이터 경로 패턴 (예: '{fish}_price_features.csv')\n",
    "        output_path (str): 저장될 파일 경로 패턴 (예: 'cleaned_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # 파일 경로 설정\n",
    "            input_file = input_path.format(fish=fish)\n",
    "            output_file = output_path.format(fish=fish)\n",
    "\n",
    "            # 데이터 로드\n",
    "            df = pd.read_csv(input_file)\n",
    "\n",
    "\n",
    "            # 첫 번째로 NULL 값이 없는 행 찾기\n",
    "            first_valid_index = df.dropna().index[1]\n",
    "\n",
    "            # 마지막으로 NULL 값이 없는 행 찾기\n",
    "            last_valid_index = df.dropna().index[-1]\n",
    "\n",
    "            # NULL 값이 없는 첫 번째 행부터 마지막 행까지 데이터 선택\n",
    "            df_cleaned = df.loc[first_valid_index:last_valid_index]\n",
    "\n",
    "            # 결과 저장\n",
    "            df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"✅ 생성된 파일: {output_file} (전체 행 수: {len(df_cleaned)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {fish} 데이터 처리 실패: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 생성된 파일: notnull_광어_price_features.csv (전체 행 수: 26555)\n",
      "✅ 생성된 파일: notnull_농어_price_features.csv (전체 행 수: 25286)\n",
      "✅ 생성된 파일: notnull_대게_price_features.csv (전체 행 수: 21516)\n",
      "✅ 생성된 파일: notnull_방어_price_features.csv (전체 행 수: 9708)\n",
      "✅ 생성된 파일: notnull_연어_price_features.csv (전체 행 수: 23975)\n",
      "✅ 생성된 파일: notnull_우럭_price_features.csv (전체 행 수: 5630)\n",
      "✅ 생성된 파일: notnull_참돔_price_features.csv (전체 행 수: 23354)\n"
     ]
    }
   ],
   "source": [
    "#  모든 수산물에 대해 반복 실행\n",
    "fish_list = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "input_path = \"{fish}_price_features.csv\"\n",
    "output_path = \"notnull_{fish}_price_features.csv\"  # {fish}_price_features_notnull.csv\n",
    "\n",
    "clean_fish_data(fish_list, input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-2. 시장별 처음 판매일 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 광어: 결측치 있는 행 제거 완료 (남은 행 수: 26555)\n",
      "✅ 농어: 결측치 있는 행 제거 완료 (남은 행 수: 25286)\n",
      "✅ 대게: 결측치 있는 행 제거 완료 (남은 행 수: 21516)\n",
      "✅ 방어: 결측치 있는 행 제거 완료 (남은 행 수: 9708)\n",
      "✅ 연어: 결측치 있는 행 제거 완료 (남은 행 수: 23975)\n",
      "✅ 우럭: 결측치 있는 행 제거 완료 (남은 행 수: 5630)\n",
      "✅ 참돔: 결측치 있는 행 제거 완료 (남은 행 수: 23354)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_missing_values(fish_list, file_path_pattern):\n",
    "    \"\"\"\n",
    "    여러 수산물 데이터에서 결측치가 있는 행을 삭제하고 기존 파일에 덮어쓰는 함수.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): 수산물 목록 (예: ['광어', '농어', ...])\n",
    "        file_path_pattern (str): 처리할 파일 경로 패턴 (예: 'notnull_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # 파일 경로 설정\n",
    "            file_path = file_path_pattern.format(fish=fish)\n",
    "\n",
    "            # 데이터 로드\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # 결측치가 있는 행 제거\n",
    "            df_cleaned = df.dropna()\n",
    "\n",
    "            # 기존 파일에 덮어쓰기 저장\n",
    "            df_cleaned.to_csv(file_path, index=False)\n",
    "\n",
    "            print(f\"✅ {fish}: 결측치 있는 행 제거 완료 (남은 행 수: {len(df_cleaned)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {fish} 데이터 처리 실패: {e}\")\n",
    "\n",
    "# ✅ 모든 수산물에 대해 실행\n",
    "fish_list = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "file_path_pattern = \"notnull_{fish}_price_features.csv\"\n",
    "\n",
    "remove_missing_values(fish_list, file_path_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11-1. 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 광어: 결측치 없음\n",
      "✅ 농어: 결측치 없음\n",
      "✅ 대게: 결측치 없음\n",
      "✅ 방어: 결측치 없음\n",
      "✅ 연어: 결측치 없음\n",
      "✅ 우럭: 결측치 없음\n",
      "✅ 참돔: 결측치 없음\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_missing_values(fish_list, file_path_pattern):\n",
    "    \"\"\"\n",
    "    여러 수산물 데이터에서 결측치 여부를 검사하는 함수.\n",
    "\n",
    "    Args:\n",
    "        fish_list (list): 수산물 목록 (예: ['광어', '농어', ...])\n",
    "        file_path_pattern (str): 검사할 파일 경로 패턴 (예: 'notnull_{fish}_price_features.csv')\n",
    "    \"\"\"\n",
    "    for fish in fish_list:\n",
    "        try:\n",
    "            # 파일 경로 설정\n",
    "            file_path = file_path_pattern.format(fish=fish)\n",
    "\n",
    "            # 데이터 로드\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # 결측치 확인\n",
    "            missing_values = df.isnull().sum()\n",
    "            total_missing = missing_values.sum()\n",
    "\n",
    "            # 결과 출력\n",
    "            if total_missing > 0:\n",
    "                print(f\"⚠️ {fish}: 총 {total_missing}개의 결측치 발견!\")\n",
    "                print(missing_values[missing_values > 0])\n",
    "            else:\n",
    "                print(f\"✅ {fish}: 결측치 없음\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {fish} 데이터 검사 실패: {e}\")\n",
    "            \n",
    "# ✅ 모든 수산물에 대해 결측치 검사 실행\n",
    "fish_list = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "file_path_pattern = \"notnull_{fish}_price_features.csv\"\n",
    "\n",
    "check_missing_values(fish_list, file_path_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 중간 과정 파일 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: _economic_indicators_.csv\n",
      "File not found: filled_economic_indicators_.csv\n",
      "File not found: expanded_economic_indicators_.csv\n",
      "Deleted: 그룹화_nst_참돔_trend.csv\n",
      "Deleted: merged_trends.csv\n",
      "Deleted: weatherdata_processed.csv\n",
      "Deleted: merged_all_data.csv\n",
      "Deleted: filled_merged_all_data.csv\n",
      "Deleted: timelagged_features.csv\n",
      "Deleted: timelagged_features1.csv\n",
      "Deleted: final_timelagged_features.csv\n",
      "Deleted: 참돔_timelagged_features.csv\n",
      "File not found: item_price_lag.csv\n",
      "File not found: item_price_lag_deleted.csv\n",
      "Deleted: item_price_lag_filled_deleted.csv\n",
      "Deleted: item_price_oneHot.csv\n",
      "Deleted: 참돔_price_oneHot.csv\n",
      "Deleted: 참돔_price_features.csv\n",
      "Deleted: 그룹화_nst_광어_trend.csv\n",
      "Deleted: 광어_timelagged_features.csv\n",
      "Deleted: 광어_price_oneHot.csv\n",
      "Deleted: 광어_price_features.csv\n",
      "Deleted: 그룹화_nst_농어_trend.csv\n",
      "Deleted: 농어_timelagged_features.csv\n",
      "Deleted: 농어_price_oneHot.csv\n",
      "Deleted: 농어_price_features.csv\n",
      "Deleted: 그룹화_nst_대게_trend.csv\n",
      "Deleted: 대게_timelagged_features.csv\n",
      "Deleted: 대게_price_oneHot.csv\n",
      "Deleted: 대게_price_features.csv\n",
      "Deleted: 그룹화_nst_방어_trend.csv\n",
      "Deleted: 방어_timelagged_features.csv\n",
      "Deleted: 방어_price_oneHot.csv\n",
      "Deleted: 방어_price_features.csv\n",
      "Deleted: 그룹화_nst_연어_trend.csv\n",
      "Deleted: 연어_timelagged_features.csv\n",
      "Deleted: 연어_price_oneHot.csv\n",
      "Deleted: 연어_price_features.csv\n",
      "Deleted: 그룹화_nst_우럭_trend.csv\n",
      "Deleted: 우럭_timelagged_features.csv\n",
      "Deleted: 우럭_price_oneHot.csv\n",
      "Deleted: 우럭_price_features.csv\n",
      "File not found: 그룹화_nst_참돔_trend.csv\n",
      "File not found: 참돔_timelagged_features.csv\n",
      "File not found: 참돔_price_oneHot.csv\n",
      "File not found: 참돔_price_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 삭제할 어종 리스트\n",
    "fish_list = ['광어', '농어', '대게', '방어', '연어', '우럭', '참돔']\n",
    "\n",
    "remove_list = [\n",
    "    '_economic_indicators_.csv',\n",
    "\t'filled_economic_indicators_.csv',\n",
    "\t'expanded_economic_indicators_.csv',\n",
    "\tf'그룹화_nst_{fish}_trend.csv',\n",
    "\t'merged_trends.csv',\n",
    "\t'weatherdata_processed.csv',\n",
    "\t'merged_all_data.csv',\n",
    "\t'filled_merged_all_data.csv',\n",
    "\t'timelagged_features.csv',\n",
    "\t'timelagged_features1.csv',\n",
    "\t'final_timelagged_features.csv',\n",
    "\tf'{fish}_timelagged_features.csv',\n",
    "\t'item_price_lag.csv',\n",
    "\t'item_price_lag_deleted.csv',\n",
    "\t'item_price_lag_filled_deleted.csv',\n",
    "\t'item_price_oneHot.csv',\n",
    "\tf'{fish}_price_oneHot.csv',\n",
    "\tf'{fish}_price_features.csv'\n",
    "]\n",
    "\n",
    "# 어종(fish)별 파일 추가\n",
    "for fish in fish_list:\n",
    "    remove_list.extend([\n",
    "        f'그룹화_nst_{fish}_trend.csv',\n",
    "        f'{fish}_timelagged_features.csv',\n",
    "        f'{fish}_price_oneHot.csv',\n",
    "        f'{fish}_price_features.csv',\n",
    "    ])\n",
    "\n",
    "# 리스트에 있는 파일들을 하나씩 삭제\n",
    "for file in remove_list:\n",
    "    if os.path.exists(file):  # 파일이 존재하는지 확인\n",
    "        os.remove(file)\n",
    "        print(f\"Deleted: {file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
