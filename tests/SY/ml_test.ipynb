{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('../../data/features/final_oneHot/광어_price_features_oneHot.csv')\n",
    "X = df.drop(['date', 'item', 'avgPrice'], axis=1)\n",
    "y = df['avgPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 계산 함수\n",
    "def calculate_metrics(y_true, y_pred, training_time):\n",
    "   mae = mean_absolute_error(y_true, y_pred)\n",
    "   mse = mean_squared_error(y_true, y_pred)\n",
    "   rmse = np.sqrt(mse)\n",
    "   r2 = r2_score(y_true, y_pred)\n",
    "   rmsle = np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "   mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "   \n",
    "   return {\n",
    "       'MAE': mae,\n",
    "       'MSE': mse,\n",
    "       'RMSE': rmse,\n",
    "       'R2': r2,\n",
    "       'RMSLE': rmsle,\n",
    "       'MAPE': mape,\n",
    "       'Training_Time': training_time\n",
    "   }\n",
    "\n",
    "# 하이퍼파라미터 그리드\n",
    "rf_params = {\n",
    "   'n_estimators': [100, 200, 300],\n",
    "   'max_depth': [10, 20, 30],\n",
    "   'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "   'n_estimators': [100, 200, 300],\n",
    "   'max_depth': [10, 20, 30],\n",
    "   'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "   'iterations': [100, 200, 300],\n",
    "   'depth': [6, 8, 10],\n",
    "   'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "   'n_estimators': [100, 200, 300],\n",
    "   'max_depth': [6, 8, 10],\n",
    "   'learning_rate': [0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest...\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5501\n",
      "[LightGBM] [Info] Number of data points in the train set: 14692, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 32186.028791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5551\n",
      "[LightGBM] [Info] Number of data points in the train set: 17629, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 32807.715696\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5564\n",
      "[LightGBM] [Info] Number of data points in the train set: 20566, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 33681.634008\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5568\n",
      "[LightGBM] [Info] Number of data points in the train set: 23503, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 34351.886142\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5571\n",
      "[LightGBM] [Info] Number of data points in the train set: 26440, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 34974.971823\n",
      "\n",
      "Training CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "2 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\miniconda3\\envs\\env311_cu121\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.50714314 0.71578145        nan        nan 0.73254486 0.66667943\n",
      " 0.44608334 0.70209532 0.648023   0.66818698 0.68745271 0.65388761\n",
      " 0.35213061 0.61660069 0.56767981 0.59689384 0.61997101 0.58534762]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Moving Window 평가\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=len(X)//10)\n",
    "models = {\n",
    "   'RandomForest': (RandomForestRegressor(), rf_params),\n",
    "   'LightGBM': (lgb.LGBMRegressor(), lgb_params),\n",
    "   'CatBoost': (CatBoostRegressor(verbose=False), cat_params),\n",
    "   'XGBoost': (xgb.XGBRegressor(), xgb_params)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (model, params) in models.items():\n",
    "   print(f\"\\nTraining {name}...\")\n",
    "   fold_metrics = []\n",
    "   \n",
    "   for train_idx, test_idx in tscv.split(X):\n",
    "       X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "       y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "       \n",
    "       # 그리드서치\n",
    "       start_time = time.time()\n",
    "       grid_search = GridSearchCV(model, params, cv=2, n_jobs=-1)\n",
    "       grid_search.fit(X_train, y_train)\n",
    "       training_time = time.time() - start_time\n",
    "       \n",
    "       # 예측 및 평가\n",
    "       y_pred = grid_search.predict(X_test)\n",
    "       metrics = calculate_metrics(y_test, y_pred, training_time)\n",
    "       fold_metrics.append(metrics)\n",
    "   \n",
    "   # 평균 지표 계산\n",
    "   avg_metrics = {metric: np.mean([fold[metric] for fold in fold_metrics]) \n",
    "                 for metric in fold_metrics[0].keys()}\n",
    "   results[name] = avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest (Random Split)...\n",
      "\n",
      "Training LightGBM (Random Split)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5568\n",
      "[LightGBM] [Info] Number of data points in the train set: 26439, number of used features: 32\n",
      "[LightGBM] [Info] Start training from score 35557.104467\n",
      "\n",
      "Training CatBoost (Random Split)...\n",
      "\n",
      "Training XGBoost (Random Split)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 랜덤 분할 평가\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "random_results = {}\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "   print(f\"\\nTraining {name} (Random Split)...\")\n",
    "   start_time = time.time()\n",
    "   grid_search = GridSearchCV(model, params, cv=3, n_jobs=-1)\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   training_time = time.time() - start_time\n",
    "   \n",
    "   y_pred = grid_search.predict(X_test)\n",
    "   metrics = calculate_metrics(y_test, y_pred, training_time)\n",
    "   random_results[name] = metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "time_series_results = pd.DataFrame(results)\n",
    "random_split_results = pd.DataFrame(random_results)\n",
    "\n",
    "time_series_results.to_csv('time_series_evaluation.csv')\n",
    "random_split_results.to_csv('random_split_evaluation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
