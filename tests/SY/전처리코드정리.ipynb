{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 필요 파일 목록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  'nst_{fish}_trend_2025-01-17.csv'\n",
    "*  'forecast_agg.csv'\n",
    "* 'item_price_lag_filled.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 경제지표 데이터 전처리\n",
    "\n",
    "filled_economic_indicators.csv 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. yfinance에서 데이터 불러오기\n",
    "\n",
    "* 입력 파일 :  없음\n",
    "* 출력 파일 : '_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'today'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 시작일과 종료일 설정\u001b[39;00m\n\u001b[0;32m      6\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoday\u001b[49m()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 데이터를 저장할 빈 데이터프레임 생성\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'today'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "# 시작일과 종료일 설정\n",
    "start_date = '2015-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# 데이터를 저장할 빈 데이터프레임 생성\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# 각 지표의 티커 심볼 정의\n",
    "tickers = {\n",
    "    'KOSPI': '^KS11',  # KOSPI\n",
    "    'USD/KRW': 'KRW=X',  # 원달러 환율\n",
    "    'WTI': 'CL=F',  # WTI 원유 선물\n",
    "    'VIX': '^VIX',  # VIX 지수\n",
    "    'Gold': 'GC=F',  # 금 선물\n",
    "    'Silver': 'SI=F',  # 은 선물\n",
    "    'MOVE' : '^MOVE' # MOVE Index\n",
    "}\n",
    "\n",
    "# 각 티커에 대해 데이터 다운로드\n",
    "for name, ticker in tickers.items():\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date)\n",
    "        # 종가만 사용\n",
    "        df_final[name] = df['Close']\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "\n",
    "# 결측치 처리\n",
    "df_final = df_final.fillna(method='ffill')\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# 기본 통계 확인\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_filename = '_economic_indicators_.csv'\n",
    "df_final.to_csv(output_filename)\n",
    "print(f\"\\nData saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. 경제지표 데이터 결측치 채우기 \n",
    "\n",
    "* 입력 파일 : '_economic_indicators_.csv'\n",
    "* 출력 파일 : 'filled_economic_indicators_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '_economic_indicators_.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_economic_indicators_.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Date 컬럼을 datetime으로 변환\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\env311_conda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\env311_conda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\env311_conda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\env311_conda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\env311_conda\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '_economic_indicators_.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('_economic_indicators_.csv', encoding='utf-8')\n",
    "\n",
    "# Date 컬럼을 datetime으로 변환\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 시작일과 마지막일 추출\n",
    "start_date = df['Date'].min()\n",
    "end_date = df['Date'].max()\n",
    "\n",
    "# 모든 날짜가 포함된 데이터프레임 생성\n",
    "all_dates = pd.DataFrame(\n",
    "    {'Date': pd.date_range(start_date, end_date, freq='D')}\n",
    ")\n",
    "\n",
    "# 기존 데이터와 병합\n",
    "filled_df = pd.merge(all_dates, df, on='Date', how='left')\n",
    "\n",
    "# 결측치를 이전 값으로 채우기\n",
    "filled_df = filled_df.ffill()\n",
    "\n",
    "# 결과 저장\n",
    "filled_df.to_csv('filled_economic_indicators.csv', index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"처리된 데이터 샘플:\")\n",
    "print(filled_df.head())\n",
    "print(\"\\n결측치 확인:\")\n",
    "print(filled_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 트렌드 데이터 어종별로 그룹화\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 그룹화 함수 생성 및 저장\n",
    "\n",
    "* 입력 파일 : 'nst_{fish}_trend_2025-01-17.csv'\n",
    "* 출력 파일 : '그룹화_nst_{fish}_trend_2025-01-17.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_age_groups(file_path):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # 연령대 그룹 매핑 딕셔너리 생성\n",
    "    age_mapping = {\n",
    "        '19_24': '20대',\n",
    "        '25_29': '20대',\n",
    "        '30_34': '30대',\n",
    "        '35_39': '30대',\n",
    "        '40_44': '40대',\n",
    "        '45_49': '40대',\n",
    "        '50_54': '50대',\n",
    "        '55_59': '50대',\n",
    "        '60_80': '60대 이상'\n",
    "    }\n",
    "    \n",
    "    # 원하는 연령대만 필터링\n",
    "    df = df[df['age'].isin(age_mapping.keys())]\n",
    "    \n",
    "    # 새로운 연령대 컬럼 생성\n",
    "    df['age_group'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # 일자별, 새로운 연령대별로 score 합산\n",
    "    result = df.groupby(['date', 'name', 'age_group'])['score'].sum().reset_index()\n",
    "    \n",
    "    # 피벗 테이블로 변환하여 보기 좋게 정리\n",
    "    pivot_result = result.pivot(index=['date', 'name'], \n",
    "                              columns='age_group', \n",
    "                              values='score').reset_index()\n",
    "    \n",
    "    # 컬럼 순서 정리\n",
    "    column_order = [ 'date', 'name', '20대', '30대', '40대', '50대', '60대 이상']\n",
    "    pivot_result = pivot_result[column_order]\n",
    "    \n",
    "    return pivot_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error processing 광어: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 농어: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 대게: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 방어: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 연어: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 우럭: process_age_groups() takes 1 positional argument but 2 were given\n",
      "❌ Error processing 참돔: process_age_groups() takes 1 positional argument but 2 were given\n",
      "\n",
      "📊 최종 처리된 결과 미리보기 (각각 상위 3개 행)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  수산물 종류별 파일 경로 및 이름 설정\n",
    "fish_files = {\n",
    "    '광어': '../../data/raw/nst_광어_trend_2025-01-17.csv',\n",
    "    '농어': '../../data/raw/nst_농어_trend_2025-01-17.csv',\n",
    "    '대게': '../../data/raw/nst_대게_trend_2025-01-17.csv',\n",
    "    '방어': '../../data/raw/nst_방어_trend_2025-01-17.csv',\n",
    "    '연어': '../../data/raw/nst_연어_trend_2025-01-17.csv',\n",
    "    '우럭': '../../data/raw/nst_우럭_trend_2025-01-17.csv',\n",
    "    '참돔': '../../data/raw/nst_참돔_trend_2025-01-17.csv'\n",
    "}\n",
    "\n",
    "#  개별 파일 저장 + 결과 리스트로 출력 준비\n",
    "all_results = {}\n",
    "\n",
    "#  모든 파일 처리\n",
    "for fish, file_path in fish_files.items():\n",
    "    try:\n",
    "        processed_df = process_age_groups(file_path, fish)\n",
    "\n",
    "        # 개별 파일 저장\n",
    "        output_filename = f'그룹화_nst_{fish}_trend_2025-01-17.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        # 결과 리스트에 추가 (첫 3개 행만 저장)\n",
    "        all_results[fish] = processed_df.head(3)\n",
    "\n",
    "        print(f\"✅ '{output_filename}' 저장 완료!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {fish}: {e}\")\n",
    "\n",
    "#  최종 처리된 결과 출력 (각각 3개 행만 출력)\n",
    "print(\"\\n📊 최종 처리된 결과 미리보기 (각각 상위 3개 행)\\n\")\n",
    "for fish, df_sample in all_results.items():\n",
    "    print(f\"{fish} 데이터 샘플:\")\n",
    "    print(df_sample)\n",
    "    print(\"-\" * 50)  # 구분선 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. merged_trends.csv 저장\n",
    "\n",
    "* 입력 파일 : '그룹화_nst_{fish}_trend_2025-01-17.csv'\n",
    "* 출력 파일 : 'merged_trends.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fish_trends(file_paths, output_path):\n",
    "   # 첫 번째 파일 로드 및 name 컬럼 제거\n",
    "   merged_df = pd.read_csv(file_paths[0], encoding='utf-8')\n",
    "   merged_df = merged_df.drop('name', axis=1)\n",
    "   merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "   \n",
    "   # 나머지 파일들 병합\n",
    "   for file_path in file_paths[1:]:\n",
    "       df = pd.read_csv(file_path, encoding='utf-8')\n",
    "       df = df.drop('name', axis=1)\n",
    "       df['date'] = pd.to_datetime(df['date'])\n",
    "       merged_df = pd.merge(merged_df, df, on='date', how='outer')\n",
    "   \n",
    "   # 날짜순 정렬\n",
    "   result = merged_df.sort_values('date')\n",
    "   \n",
    "   # CSV 저장\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시:\n",
    "files = [\n",
    "    '그룹화_nst_광어_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_농어_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_대게_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_방어_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_연어_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_우럭_trend_2025-01-17.csv', \n",
    "    '그룹화_nst_참돔_trend_2025-01-17.csv'\n",
    "    ]\n",
    "\n",
    "result = merge_fish_trends(files, 'merged_trends.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 기상 데이터 전처리\n",
    "\n",
    "* 입력 파일 : 'forecast_agg.csv'\n",
    "* 출력 파일 : 'weatherdata_processed.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "\n",
    "def create_weather_columns(df, station_cols, output_path):\n",
    "   result_df = df.copy()\n",
    "   result_df['일시'] = pd.to_datetime(result_df['일시'])\n",
    "   \n",
    "   final_df = pd.DataFrame({'일시': result_df['일시'].unique()})\n",
    "   \n",
    "   for station, columns in station_cols.items():\n",
    "       station_data = result_df[result_df['지점'] == station].copy()\n",
    "       \n",
    "       for orig_col, new_col in columns:\n",
    "           station_col = station_data[['일시', orig_col]].copy()\n",
    "           final_df = pd.merge(final_df, station_col.rename(columns={orig_col: new_col}), \n",
    "                             on='일시', how='left')\n",
    "   \n",
    "   # 칼럼을 가나다순으로 정렬 ('일시' 컬럼은 첫번째로 유지)\n",
    "   sorted_cols = ['일시'] + sorted([col for col in final_df.columns if col != '일시'])\n",
    "   result = final_df[sorted_cols].sort_values('일시')\n",
    "   \n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('forecast_agg.csv', encoding='utf-8')\n",
    "\n",
    "# 피쳐 선택\n",
    "station_columns = {\n",
    "\t22105: [\n",
    "\t\t('기온', '광어_기온_22105'),\n",
    "\t\t('기온', '농어_기온_22105'),\n",
    "\t\t('기온', '대게_기온_22105'),\n",
    "\t\t('파주기', '대게_파주기_22105'),\n",
    "\t\t('파주기', '방어_파주기_22105'),\n",
    "\t\t('기온', '연어_기온_22105'),\n",
    "\t\t('습도', '연어_습도_22105')\n",
    "\t],\n",
    "    \n",
    "\t22107: [\n",
    "\t\t('수온', '광어_수온_22107'),\n",
    "\t\t('수온', '농어_수온_22107'),\n",
    "\t\t('수온', '방어_수온_22107'),\n",
    "\t\t('수온', '연어_수온_22107'),\n",
    "\t\t('수온', '참돔_수온_22107')\n",
    "\t],\n",
    "\n",
    "\t22186: [\n",
    "\t\t('습도', '광어_습도_22186'),\n",
    "\t\t('습도', '농어_습도_22186'),\n",
    "\t\t('기온', '우럭_기온_22186'),\n",
    "\t\t('수온', '우럭_수온_22186')\n",
    "\t\t],\n",
    "        \n",
    "\t22188: [\n",
    "\t\t('수온', '대게_수온_22188'),\n",
    "\t\t('습도', '대게_습도_22188')\n",
    "\t\t],        \n",
    "\n",
    "\t22189: [\n",
    "\t\t('파주기', '우럭_파주기_22189')\n",
    "\t\t],       \n",
    "\n",
    "\t22190: [\n",
    "\t\t('파주기', '광어_파주기_22190'),\n",
    "        ('파주기', '농어_파주기_22190'),\n",
    "        ('기온', '방어_기온_22190'),\n",
    "        ('습도', '방어_습도_22190'),\n",
    "        ('파주기', '연어_파주기_22190'),\n",
    "        ('습도', '우럭_습도_22190'),\n",
    "        ('기온', '참돔_기온_22190'),\n",
    "        ('습도', '참돔_습도_22190'),\n",
    "        ('파주기', '참돔_파주기_22190')\n",
    "\t\t]  \n",
    "\n",
    "\t}\n",
    "\n",
    "result = create_weather_columns(df, station_columns, 'weatherdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 변수 하나로 합치기\n",
    "\n",
    "* 입력 파일 : 'filled_economic_indicators_.csv',  'merged_trends.csv', 'weatherdata_processed.csv'\n",
    "* 출력 파일 : 'merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(output_path='merged_all_data.csv'):\n",
    "   # 각 파일 로드 및 전처리\n",
    "   \n",
    "   economic = pd.read_csv('_filled_economic_indicators.csv', encoding='utf-8')\n",
    "   economic['Date'] = pd.to_datetime(economic['Date'])\n",
    "   economic = economic.rename(columns={'Date': '날짜'})\n",
    "   \n",
    "   trends = pd.read_csv('merged_trends.csv', encoding='utf-8')\n",
    "   trends['date'] = pd.to_datetime(trends['date'])\n",
    "   trends = trends.rename(columns={'date': '날짜'})\n",
    "   \n",
    "   weather = pd.read_csv('weatherdata_processed.csv', encoding='utf-8')\n",
    "   weather['date'] = pd.to_datetime(weather['date'])\n",
    "   weather = weather.rename(columns={'date': '날짜'})\n",
    "   \n",
    "   # 모든 날짜 추출\n",
    "   all_dates = pd.concat([\n",
    "        economic['날짜'], \n",
    "        trends['날짜'], \n",
    "       weather['날짜']\n",
    "   ]).unique()\n",
    "   \n",
    "   # 날짜 기준 데이터프레임 생성\n",
    "   date_df = pd.DataFrame({'날짜': all_dates})\n",
    "   date_df = date_df.sort_values('날짜')\n",
    "   \n",
    "   # 데이터 병합\n",
    "   dfs = [\n",
    "\t\tdate_df, \n",
    "\t\teconomic, \n",
    "\t\ttrends,\n",
    "\t\tweather\n",
    "   ]\n",
    "   \n",
    "   result = dfs[0]\n",
    "   for df in dfs[1:]:\n",
    "       result = pd.merge(result, df, on='날짜', how='left')\n",
    "   \n",
    "   # 결과 저장\n",
    "   result.to_csv(output_path, index=False, encoding='utf-8')\n",
    "   return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = merge_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 결측치 채우기\n",
    "\n",
    "* 입력 파일 : 'merged_all_data.csv'\n",
    "* 출력 파일 : 'filled_merged_all_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fill_missing_values(file_path, output_path):\n",
    "    \"\"\"\n",
    "    CSV 파일에서 결측치를 `ffill`을 사용하여 채우되, 이전 값이 없으면 그대로 둠.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 입력 CSV 파일 경로.\n",
    "        output_path (str): 처리된 데이터를 저장할 경로.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 결측치가 채워진 데이터프레임.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV 파일 로드\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 결측치 확인 (처리 전)\n",
    "    missing_before = df.isnull().sum()\n",
    "\n",
    "    # 결측치 `ffill`로 채우기 (이전 값이 없으면 그대로 둠)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # 결측치 확인 (처리 후)\n",
    "    missing_after = df.isnull().sum()\n",
    "\n",
    "    # 변경된 결측치 개수 출력\n",
    "    missing_summary = pd.DataFrame({'Before': missing_before, 'After': missing_after})\n",
    "    print(\"\\n🔍 결측치 처리 전후 비교:\")\n",
    "    print(missing_summary)\n",
    "\n",
    "    # 처리된 데이터 저장\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ 처리된 데이터가 '{output_path}'에 저장되었습니다.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'merged_all_data.csv'\n",
    "output_path = 'filled_merged_all_data.csv'\n",
    "\n",
    "fill_missing_values(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 타임래그 적용하기 \n",
    "\n",
    "* 입력 파일 : 'filled_merged_all_data.csv'\n",
    "* 출력 파일 : 'timelagged_features.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. 설정된 타임래그 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('filled_merged_all_data.csv', encoding='utf-8')\n",
    "df['date'] = pd.to_datetime(df['날짜'])\n",
    "\n",
    "# 각 칼럼별 lag 일수 정의\n",
    "lag_days = {\n",
    "\n",
    "'연어_거래량(톤)' : 374,\n",
    "'연어_가격(NOK/kg)' : 32,\n",
    "\n",
    "'광어_대'  : 64,\n",
    "'방어(자연)_대_가락' : 5,\n",
    "'참돔_대_가락' : 1,\n",
    "\n",
    "'광어_KOSPI' : 136,\n",
    "'광어_USD/KRW' : 1,\n",
    "'광어_WTI' : 1,\n",
    "'광어_VIX' : 399,\n",
    "'광어_Gold' : 314,\n",
    "'광어_Silver' : 238,\n",
    "'광어_MOVE' : 18,\n",
    "\n",
    "'농어_KOSPI' : 179,\n",
    "'농어_USD/KRW' : 1,\n",
    "'농어_WTI' : 100,\n",
    "'농어_VIX' : 391,\n",
    "'농어_Gold' : 361,\n",
    "'농어_Silver' : 290,\n",
    "'농어_MOVE' : 1,\n",
    "\n",
    "'대게_KOSPI' : 148,\n",
    "'대게_USD/KRW' : 90,\n",
    "'대게_WTI' : 91,\n",
    "# '대게_VIX' : \n",
    "'대게_Gold' : 177,\n",
    "'대게_Silver' : 177,\n",
    "# '대게_MOVE' : \n",
    "\n",
    "'방어_KOSPI' : 282,\n",
    "'방어_USD/KRW' : 387,\n",
    "'방어_WTI' : 399,\n",
    "'방어_VIX' : 133,\n",
    "'방어_Gold' : 1,\n",
    "'방어_Silver' : 1,\n",
    "'방어_MOVE' : 381,\n",
    "\n",
    "'연어_KOSPI' : 329,\n",
    "'연어_USD/KRW' : 1,\n",
    "'연어_WTI' : 199,\n",
    "'연어_VIX' : 399,\n",
    "'연어_Gold' : 1,\n",
    "'연어_Silver' : 399,\n",
    "'연어_MOVE' : 71,\n",
    "\n",
    "'우럭_KOSPI' : 155,\n",
    "'우럭_USD/KRW' : 121,\n",
    "'우럭_WTI' : 14,\n",
    "'우럭_VIX' : 38,\n",
    "'우럭_Gold' : 146,\n",
    "'우럭_Silver' : 125,\n",
    "# '우럭_MOVE' : \n",
    "\n",
    "'참돔_KOSPI' : 399,\n",
    "'참돔_USD/KRW' : 11,\n",
    "'참돔_WTI' : 150,\n",
    "# '참돔_VIX' : \n",
    "# '참돔_Gold' : \n",
    "# '참돔_Silver' : \n",
    "'참돔_MOVE' : 201,\n",
    " \n",
    "'광어_20대' : 250,\n",
    "'광어_30대' : 317,\n",
    "'광어_40대' : 330,\n",
    "'광어_50대' : 395,\n",
    "'광어_60대이상' : 339,\n",
    "\n",
    "'농어_20대' : 305,\n",
    "'농어_30대' : 307,\n",
    "'농어_40대' : 309,\n",
    "'농어_50대' : 309,\n",
    "'농어_60대이상' : 273,\n",
    "\n",
    "'대게_20대' : 273,\n",
    "'대게_30대' : 370,\n",
    "'대게_40대' : 5,\n",
    "'대게_50대' : 5,\n",
    "'대게_60대이상' : 5,\n",
    "\n",
    "'방어_20대' : 22,\n",
    "'방어_30대' : 26,\n",
    "'방어_40대' : 256,\n",
    "'방어_50대' : 256,\n",
    "'방어_60대이상' : 26,\n",
    "\n",
    "'연어_20대' : 20,\n",
    "'연어_30대' : 15,\n",
    "'연어_40대' : 21,\n",
    "'연어_50대' : 15,\n",
    "'연어_60대이상' : 397,\n",
    "\n",
    "'우럭_20대' : 22,\n",
    "'우럭_30대' : 354,\n",
    "'우럭_40대' : 354,\n",
    "'우럭_50대' : 220,\n",
    "'우럭_60대이상' : 219,\n",
    "\n",
    "'참돔_20대' : 197,\n",
    "'참돔_30대' : 167,\n",
    "'참돔_40대' : 278,\n",
    "'참돔_50대' : 193,\n",
    "'참돔_60대이상' : 14,\n",
    "\n",
    "\n",
    "'광어_기온_22105' : 97,\n",
    "'광어_수온_22107' : 79,\n",
    "'광어_습도_22186' : 349,\n",
    "'광어_파주기_22190' : 103,\n",
    "\n",
    "'농어_기온_22105' : 103,\n",
    "'농어_수온_22107' : 81,\n",
    "'농어_습도_22186' : 333,\n",
    "'농어_파주기_22190' : 115,\n",
    "\n",
    "'대게_기온_22105' : 150,\n",
    "'대게_수온_22188' : 140,\n",
    "'대게_습도_22188' : 355,\n",
    "'대게_파주기_22105' : 174,\n",
    "\n",
    "'방어_기온_22190' : 114,\n",
    "'방어_수온_22107' : 118,\n",
    "'방어_습도_22190' : 158,\n",
    "'방어_파주기_22105' : 191,\n",
    "\n",
    "'연어_기온_22105' : 394,\n",
    "'연어_수온_22107' : 341,\n",
    "'연어_습도_22105' : 8,\n",
    "'연어_파주기_22190' : 9,\n",
    "\n",
    "'우럭_기온_22186' : 118,\n",
    "'우럭_수온_22186' : 113,\n",
    "'우럭_습도_22190' : 159,\n",
    "'우럭_파주기_22189' : 172,\n",
    "\n",
    "'참돔_기온_22190' : 91,\n",
    "'참돔_수온_22107' : 72,\n",
    "'참돔_습도_22190' : 115,\n",
    "'참돔_파주기_22190' : 107\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# 각 칼럼에 대해 지정된 lag 적용\n",
    "for col, lag in lag_days.items():\n",
    "   if col in df.columns:\n",
    "       df[f'{col}_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv('timelagged_features.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2 트렌드 데이터 lag 1 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 가격 데이터 - 노량진 2층 버리기\n",
    "\n",
    "* 입력 파일 : 'item_price_lag_filled.csv'\n",
    "* 출력 파일 : 'item_price_lag_filled_deleted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = \"item_price_lag_filled.csv\"  # 원본 데이터 파일\n",
    "output_file = \"item_price_lag_filled_deleted.csv\"  # 저장될 파일\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 'market' 칼럼에서 '노량진 2층'인 행 제거\n",
    "df = df[df['market'] != '노량진 2층']\n",
    "\n",
    "# '노량진 1층'을 '노량진시장'으로 변경\n",
    "df['market'] = df['market'].replace('노량진 1층', '노량진시장')\n",
    "\n",
    "# 처리된 데이터 저장\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 변경된 데이터 확인\n",
    "print(\"\\n✅ 변경된 데이터 (상위 5개 행):\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\n📁 처리된 파일이 '{output_file}'로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 시장 데이터 원핫 인코딩\n",
    "\n",
    "* 입력 파일 : 'item_price_lag_filled_deleted.csv'\n",
    "* 출력 파일 : 'item_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import DateTime\n",
    "\n",
    "def transform_market_data(file_path):\n",
    "    # 데이터 읽기\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 시장별 더미변수 생성 (0, 1로 인코딩)\n",
    "    market_dummies = pd.get_dummies(df['market'], prefix='m').astype(int)\n",
    "    \n",
    "    # 원본 데이터와 더미변수 결합\n",
    "    result = pd.concat([\n",
    "        df[['priceDate', 'item']], \n",
    "        market_dummies,\n",
    "        df[['avgPrice', 'avgPrice_lag_1']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # 날짜와 어종으로 정렬\n",
    "    result = result.sort_values(['priceDate', 'item'])\n",
    "    \n",
    "    # 변환된 데이터 저장\n",
    "    output_file = 'item_price_oneHot.csv'\n",
    "    result.to_csv(output_file, index=False)\n",
    "    print(f\"생성된 파일: {output_file}\")\n",
    "    print(\"\\n처음 10개 컬럼:\", result.columns[:10].tolist())\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 파일: transformed_market_data.csv\n",
      "\n",
      "처음 10개 컬럼: ['priceDate', 'item', 'm_가락시장', 'm_강서농수산물시장', 'm_구리농수산물시장', 'm_노량진 1층', 'm_노량진 2층', 'm_마포농수산물시장', 'm_부산민락어민활어직판장', 'm_소래포구종합어시장']\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "df = transform_market_data('item_price_lag_filled_deleted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 가격데이터 어종별로 나누기\n",
    "\n",
    "* 입력 파일 : 'item_price_oneHot.csv'\n",
    "* 출력 파일 : '{fish}_price_oneHot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_market_data():\n",
    "    df = pd.read_csv('item_price_oneHot.csv')\n",
    "    \n",
    "    for fish in df['item'].unique():\n",
    "        fish_df = df[df['item'] == fish]\n",
    "        output_file = f'{fish}_price_oneHot.csv'\n",
    "      # fish_df = fish_df.drop('item', axis=1)\n",
    "        fish_df.to_csv(output_file, index=False)\n",
    "        print(f'{fish} 데이터 생성 완료: {len(fish_df)}개 행')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대게 데이터 생성 완료: 27677개 행\n",
      "광어 데이터 생성 완료: 31515개 행\n",
      "농어 데이터 생성 완료: 30651개 행\n",
      "연어 데이터 생성 완료: 29588개 행\n",
      "참돔 데이터 생성 완료: 27149개 행\n",
      "방어 데이터 생성 완료: 11205개 행\n",
      "우럭 데이터 생성 완료: 6081개 행\n"
     ]
    }
   ],
   "source": [
    "split_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 최종 데이터 합치기\n",
    "\n",
    "* 입력 파일 : '{fish}_price_oneHot.csv', '{fish}_timelagged_features.csv'\n",
    "* 출력 파일 : '{fish}_price_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 파일: 참돔_price_features.csv\n",
      "전체 컬럼 수: 35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_price():\n",
    "    # 두 데이터셋 로드\n",
    "    features_df = pd.read_csv('../../data/features/참돔_timelagged_features.csv')\n",
    "    price_df = pd.read_csv('../../data/features/oneHot/참돔_price_oneHot.csv')\n",
    "    \n",
    "    # 날짜 컬럼명 통일\n",
    "    price_df = price_df.rename(columns={'dmdpriceDate': 'date'})\n",
    "    \n",
    "    # 날짜 기준으로 데이터 병합\n",
    "    merged_df = pd.merge(price_df, features_df, on='date', how='left')\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    output_file = '참돔_price_features.csv'\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f'생성된 파일: {output_file}')\n",
    "    print(f'전체 컬럼 수: {len(merged_df.columns)}')\n",
    "\n",
    "merge_features_price()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
