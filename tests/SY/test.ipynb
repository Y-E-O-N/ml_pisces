{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_age_groups(file_path):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # 연령대 그룹 매핑 딕셔너리 생성\n",
    "    age_mapping = {\n",
    "        '19_24': '20대',\n",
    "        '25_29': '20대',\n",
    "        '30_34': '30대',\n",
    "        '35_39': '30대',\n",
    "        '40_44': '40대',\n",
    "        '45_49': '40대',\n",
    "        '50_54': '50대',\n",
    "        '55_59': '50대',\n",
    "        '60_80': '60대 이상'\n",
    "    }\n",
    "    \n",
    "    # 원하는 연령대만 필터링\n",
    "    df = df[df['age'].isin(age_mapping.keys())]\n",
    "    \n",
    "    # 새로운 연령대 컬럼 생성\n",
    "    df['age_group'] = df['age'].map(age_mapping)\n",
    "    \n",
    "    # 일자별, 새로운 연령대별로 score 합산\n",
    "    result = df.groupby(['date', 'name', 'age_group'])['score'].sum().reset_index()\n",
    "    \n",
    "    # 피벗 테이블로 변환하여 보기 좋게 정리\n",
    "    pivot_result = result.pivot(index=['date', 'name'], \n",
    "                              columns='age_group', \n",
    "                              values='score').reset_index()\n",
    "    \n",
    "    # 컬럼 순서 정리\n",
    "    column_order = [ 'date', 'name', '20대', '30대', '40대', '50대', '60대 이상']\n",
    "    pivot_result = pivot_result[column_order]\n",
    "    \n",
    "    return pivot_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 광어\n",
    "result_df = process_age_groups('../../data/raw/nst_광어_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_광어_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 농어 \n",
    "result_df = process_age_groups('../../data/raw/nst_농어_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_농어_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대게 \n",
    "result_df = process_age_groups('../../data/raw/nst_방어_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_방어_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방어 \n",
    "result_df = process_age_groups('../../data/raw/nst_방어_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_방어_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연어\n",
    "result_df = process_age_groups('../../data/raw/nst_연어_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_연어_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우럭 \n",
    "result_df = process_age_groups('../../data/raw/nst_우럭_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_우럭_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참돔 \n",
    "result_df = process_age_groups('../../data/raw/nst_참돔_trend_2025-01-17.csv')\n",
    "result_df.to_csv('그룹화_nst_참돔_trend_2025-01-17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터가 daily_item_average_prices.csv에 저장되었습니다.\n",
      "\n",
      "처리된 데이터 미리보기:\n",
      "     priceDate item  avgPrice\n",
      "19  2015-02-25   광어   25000.0\n",
      "23  2015-02-26   광어   25000.0\n",
      "27  2015-02-27   광어   25000.0\n",
      "31  2015-02-28   광어   25000.0\n",
      "35  2015-03-01   광어   25000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4516\\2890981007.py:12: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='ffill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4516\\2890981007.py:12: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='ffill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4516\\2890981007.py:13: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='bfill')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4516\\2890981007.py:13: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_item_daily_average(file_path):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    \n",
    "    # 날짜와 어종별로 그룹화하여 시장 평균 계산\n",
    "    daily_avg = df.groupby(['priceDate', 'item'])['avgPrice'].mean().reset_index()\n",
    "    \n",
    "    # 결측치를 앞뒤 값으로 채우기\n",
    "    daily_avg = daily_avg.sort_values(['item', 'priceDate'])\n",
    "    daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='ffill')\n",
    "    daily_avg['avgPrice'] = daily_avg.groupby('item')['avgPrice'].fillna(method='bfill')\n",
    "    \n",
    "    return daily_avg\n",
    "\n",
    "# 파일 처리\n",
    "file_path = '../../data/processed/item_price_lag_filled.csv'\n",
    "result_df = calculate_item_daily_average(file_path)\n",
    "\n",
    "# 결과 저장\n",
    "output_file = 'daily_item_average_prices.csv'\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(f\"처리된 데이터가 {output_file}에 저장되었습니다.\")\n",
    "\n",
    "# 결과 미리보기\n",
    "print(\"\\n처리된 데이터 미리보기:\")\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import DateTime\n",
    "\n",
    "def transform_market_data(file_path):\n",
    "    # 데이터 읽기\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 시장별 더미변수 생성 (0, 1로 인코딩)\n",
    "    market_dummies = pd.get_dummies(df['market'], prefix='m').astype(int)\n",
    "    \n",
    "    # 원본 데이터와 더미변수 결합\n",
    "    result = pd.concat([\n",
    "        df[['priceDate', 'item']], \n",
    "        market_dummies,\n",
    "        df[['avgPrice', 'avgPrice_lag_1']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    # 날짜와 어종으로 정렬\n",
    "    result = result.sort_values(['priceDate', 'item'])\n",
    "    \n",
    "    # 변환된 데이터 저장\n",
    "    output_file = 'transformed_market_data.csv'\n",
    "    result.to_csv(output_file, index=False)\n",
    "    print(f\"생성된 파일: {output_file}\")\n",
    "    print(\"\\n처음 10개 컬럼:\", result.columns[:10].tolist())\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 파일: transformed_market_data.csv\n",
      "\n",
      "처음 10개 컬럼: ['priceDate', 'item', 'm_가락시장', 'm_강서농수산물시장', 'm_구리농수산물시장', 'm_노량진 1층', 'm_노량진 2층', 'm_마포농수산물시장', 'm_부산민락어민활어직판장', 'm_소래포구종합어시장']\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "df = transform_market_data('../../data/processed/item_price_lag_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_market_data():\n",
    "    df = pd.read_csv('item_price_oneHot.csv')\n",
    "    \n",
    "    for fish in df['item'].unique():\n",
    "        fish_df = df[df['item'] == fish]\n",
    "        output_file = f'{fish}_price_oneHot.csv'\n",
    "      # fish_df = fish_df.drop('item', axis=1)\n",
    "        fish_df.to_csv(output_file, index=False)\n",
    "        print(f'{fish} 데이터 생성 완료: {len(fish_df)}개 행')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대게 데이터 생성 완료: 27677개 행\n",
      "광어 데이터 생성 완료: 31515개 행\n",
      "농어 데이터 생성 완료: 30651개 행\n",
      "연어 데이터 생성 완료: 29588개 행\n",
      "참돔 데이터 생성 완료: 27149개 행\n",
      "방어 데이터 생성 완료: 11205개 행\n",
      "우럭 데이터 생성 완료: 6081개 행\n"
     ]
    }
   ],
   "source": [
    "split_market_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 파일: 참돔_price_features.csv\n",
      "전체 컬럼 수: 35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_features_price():\n",
    "    # 두 데이터셋 로드\n",
    "    features_df = pd.read_csv('../../data/features/참돔_timelagged_features.csv')\n",
    "    price_df = pd.read_csv('../../data/features/oneHot/참돔_price_oneHot.csv')\n",
    "    \n",
    "    # 날짜 컬럼명 통일\n",
    "    price_df = price_df.rename(columns={'dmdpriceDate': 'date'})\n",
    "    \n",
    "    # 날짜 기준으로 데이터 병합\n",
    "    merged_df = pd.merge(price_df, features_df, on='date', how='left')\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    output_file = '참돔_price_features.csv'\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f'생성된 파일: {output_file}')\n",
    "    print(f'전체 컬럼 수: {len(merged_df.columns)}')\n",
    "\n",
    "merge_features_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_noryangjin_data():\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv('광어_price_features.csv')\n",
    "    \n",
    "    # 노량진 1층과 2층 데이터 추출\n",
    "    floor1 = df[df['m_노량진 1층'] == 1]\n",
    "    floor2 = df[df['m_노량진 2층'] == 1]\n",
    "    \n",
    "    # 필요한 컬럼만 선택하여 데이터프레임 생성\n",
    "    floor1_prices = floor1[['date', 'avgPrice', 'avgPrice_lag_1']]\n",
    "    floor2_prices = floor2[['date', 'avgPrice', 'avgPrice_lag_1']]\n",
    "    \n",
    "    # 날짜별로 평균 계산\n",
    "    merged_prices = pd.merge(floor1_prices, floor2_prices, on='date', how='outer', suffixes=('_1층', '_2층'))\n",
    "    merged_prices['avgPrice'] = merged_prices[['avgPrice_1층', 'avgPrice_2층']].mean(axis=1)\n",
    "    merged_prices['avgPrice_lag_1'] = merged_prices[['avgPrice_lag_1_1층', 'avgPrice_lag_1_2층']].mean(axis=1)\n",
    "    \n",
    "    # 원본 데이터에서 노량진 1층, 2층 데이터 제거\n",
    "    df_filtered = df[~((df['m_노량진 1층'] == 1) | (df['m_노량진 2층'] == 1))].copy()\n",
    "    \n",
    "    # 새로운 노량진 데이터 생성\n",
    "    noryangjin_data = df_filtered.iloc[0:len(merged_prices)].copy()\n",
    "    noryangjin_data.loc[:, 'date'] = merged_prices['date'].values\n",
    "    noryangjin_data.loc[:, 'avgPrice'] = merged_prices['avgPrice'].values\n",
    "    noryangjin_data.loc[:, 'avgPrice_lag_1'] = merged_prices['avgPrice_lag_1'].values\n",
    "    \n",
    "    # 모든 시장 칼럼을 0으로 설정\n",
    "    market_cols = [col for col in df.columns if col.startswith('m_')]\n",
    "    for col in market_cols:\n",
    "        noryangjin_data[col] = 0\n",
    "    \n",
    "    # 노량진 칼럼 추가\n",
    "    noryangjin_data['m_노량진'] = 1\n",
    "    \n",
    "    # 기존 노량진 1층, 2층 칼럼 삭제\n",
    "    df_filtered = df_filtered.drop(['m_노량진 1층', 'm_노량진 2층'], axis=1)\n",
    "    noryangjin_data = noryangjin_data.drop(['m_노량진 1층', 'm_노량진 2층'], axis=1)\n",
    "    \n",
    "    # 데이터 합치기\n",
    "    final_df = pd.concat([df_filtered, noryangjin_data], axis=0)\n",
    "    \n",
    "    # 날짜순으로 정렬\n",
    "    final_df = final_df.sort_values('date')\n",
    "    \n",
    "    # m_노량진 칼럼 위치 조정\n",
    "    cols = final_df.columns.tolist()\n",
    "    cols.remove('m_노량진')\n",
    "    cols.insert(3, 'm_노량진')\n",
    "    final_df = final_df[cols]\n",
    "    \n",
    "    # 저장\n",
    "    final_df.to_csv('광어_price_features_noryangjin_merged.csv', index=False)\n",
    "    print('노량진 데이터 통합 및 칼럼 재정렬 완료')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노량진 데이터 통합 및 칼럼 재정렬 완료\n"
     ]
    }
   ],
   "source": [
    "merge_noryangjin_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
